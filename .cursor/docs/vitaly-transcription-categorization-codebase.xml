This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .cursor/**, .tmp/**, test/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.gitignore
.gitlab-ci.yml
.python-version
config/settings.json
Dockerfile
entrypoint.sh
healthcheck.py
inspeccion_vimeo.py
no_pubsub/run_batch_processing.py
prueba_descarga_vimeo.py
prueba_nombre_sin_descarga.py
prueba_prompt.py
README.md
requirements.txt
setup/setup_clap.py
setup/setup_db.py
setup/setup_demucs.py
setup/setup_silero_vad.py
src/classify_categories.py
src/config_loader.py
src/detect_music.py
src/download_videos.py
src/listener.py
src/prueba_download.py
src/utils_db.py
src/utils_error.py
src/utils_mixto.py
src/utils_narracion.py
src/utils_ocr.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".python-version">
3.13
</file>

<file path=".gitlab-ci.yml">
# File: .gitlab-ci.yml
variables:
  GAR_REPO: "vitaly-transcription-and-categorization"

stages:
  - trivy
  - deploy

include:
  - project: "general-tps/seguridad/pipelines"
    file:
      - "trivy.yaml"
      - "deploy.yaml"
</file>

<file path="inspeccion_vimeo.py">
import requests
import json
from urllib.parse import urlparse
from dotenv import load_dotenv
import os

# Cargar variables de entorno
load_dotenv()
ACCESS_TOKEN = os.getenv("VIMEO_ACCESS_TOKEN")

HEADERS = {
    "Authorization": f"Bearer {ACCESS_TOKEN}"
}

def get_video_id(video_url):
    path = urlparse(video_url).path
    return path.strip('/')

def inspect_video_metadata(video_url):
    video_id = get_video_id(video_url)
    api_url = f"https://api.vimeo.com/videos/{video_id}"

    try:
        response = requests.get(api_url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()

        print("\nüé¨ Metadata completa (Vimeo API):\n")
        print(json.dumps(data, indent=2, ensure_ascii=False))

        print("\nüìå Campos clave extra√≠dos:")
        print(f"- video_id: {video_id}")
        print(f"- name (t√≠tulo): {data.get('name')}")
        print(f"- duration: {data.get('duration')} segundos")
        print(f"- created_time: {data.get('created_time')}")
        print(f"- privacy: {data.get('privacy', {}).get('view')}")

        print("\n‚¨áÔ∏è  Opciones de descarga:")
        if 'download' in data:
            for i, option in enumerate(data['download'], 1):
                print(f"  [{i}] {option.get('public_name')} - {option.get('width')}px - {option.get('link')}")
        else:
            print("  ‚ö†Ô∏è  No hay opciones de descarga disponibles.")

    except requests.RequestException as e:
        print(f"‚ùå Error al consultar la API de Vimeo: {e}")

if __name__ == "__main__":
    VIDEO_URL = "https://vimeo.com/688055252"  # Cambia esto por el que quieras inspeccionar
    inspect_video_metadata(VIDEO_URL)
</file>

<file path="prueba_descarga_vimeo.py">
import requests
import os
from urllib.parse import urlparse

# Configura tu token aqu√≠
ACCESS_TOKEN = 'c99e88fa5ef5201d7e448b40a48a0801'

# URL del video que quieres descargar
VIDEO_URL = 'https://vimeo.com/688055252'

# Cabeceras para la API
HEADERS = {
    'Authorization': f'Bearer {ACCESS_TOKEN}'
}

def get_video_id(video_url):
    """Extrae el ID del video desde la URL."""
    path = urlparse(video_url).path
    return path.strip('/')

def get_download_info(video_id):
    """Obtiene el enlace de descarga y el nombre real del video usando la API de Vimeo."""
    api_url = f'https://api.vimeo.com/videos/{video_id}'
    response = requests.get(api_url, headers=HEADERS)
    response.raise_for_status()

    data = response.json()

    # Verificar si hay opciones de descarga
    if 'download' not in data or not data['download']:
        print('‚ö†Ô∏è  No hay enlaces de descarga disponibles.')
        return None, None

    download_options = data['download']
    best_option = max(download_options, key=lambda x: x.get('width', 0))

    download_url = best_option['link']
    video_name = data.get('name', 'video_descargado')

    # Limpiar el nombre para guardar como archivo
    safe_video_name = sanitize_filename(video_name) + '.mp4'

    return download_url, safe_video_name

def sanitize_filename(name):
    """Limpia el nombre del archivo eliminando caracteres no v√°lidos."""
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '')
    return name

def download_video(url, filename):
    """Descarga el video desde el enlace proporcionado."""
    print(f'Descargando {filename}...')
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    print(f'Descarga completada: {filename}')

def main():
    video_id = get_video_id(VIDEO_URL)
    download_url, filename = get_download_info(video_id)

    if not download_url:
        print('‚ùå No se pudo descargar el video.')
        return

    filename = os.path.basename(filename)
    download_video(download_url, filename)

if __name__ == '__main__':
    main()
</file>

<file path="prueba_nombre_sin_descarga.py">
import requests
from urllib.parse import urlparse

# Configura tu token aqu√≠
ACCESS_TOKEN = '8cda5e9bdf18203e5c5628c01ea0f77c'

# Cabeceras para la API
HEADERS = {
    'Authorization': f'Bearer {ACCESS_TOKEN}'
}

def get_video_id(video_url):
    """Extrae el ID del video desde la URL."""
    path = urlparse(video_url).path
    return path.strip('/')

def get_video_name(video_id):
    """Obtiene el nombre (t√≠tulo) del video usando la API de Vimeo."""
    api_url = f'https://api.vimeo.com/videos/{video_id}'
    response = requests.get(api_url, headers=HEADERS)
    response.raise_for_status()

    data = response.json()

    # Devuelve el campo 'name' (t√≠tulo)
    return data.get('name', 'Sin t√≠tulo')

def main():
    VIDEO_URL = 'https://vimeo.com/688055252'
    video_id = get_video_id(VIDEO_URL)
    video_name = get_video_name(video_id)
    
    print(f"Nombre del video: {video_name}")

if __name__ == '__main__':
    main()
</file>

<file path="prueba_prompt.py">
import os, json, time, sys, re
from openai import AzureOpenAI
from typing import Optional, Dict, Any, List, Union
from dotenv import load_dotenv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.utils_db import connect_db, get_categories, get_final_transcript, store_keywords_categories, store_suggested_title_description

# ==================== 
# CARGAR CONFIGURACI√ìN 
# ====================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "."))
AUDIO_DIR = os.path.join(BASE_DIR, "tmp/audio")
CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

GPT_TEMPERATURE_CATEGORIES = config["gpt"]["temperature_categories"]
GPT_PROMPT_CATEGORIES = config["gpt"]["prompt_categories"]

# =======================
# CONFIGURACI√ìN AZURE
# =======================
load_dotenv()
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Checkear variables
if not all([endpoint, deployment, api_key, api_version]):
    raise ValueError("‚ùå Error al cargar variables de entorno para Azure OpenAI.")

# Iniciar cliente de Azure OpenAI
client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version=api_version
)


# -----------------------------
# FUNCI√ìN: obtain_keywords_categories
# -----------------------------
def extract_json_block(text: str) -> str:
    """
    Extrae el bloque JSON de una respuesta con formato Markdown.
    """
    match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        return match.group(1)
    return text.strip() 

def obtain_keywords_categories(final_transcript: str, video_title: str, categories: List[str]
                               ) -> Optional[Dict[str, Union[str, float, int, List[str]]]]:
    """
    Llama a Azure OpenAI para obtener keywords y categor√≠as relevantes de un video.
    """
    categories_str = ", ".join(categories)
    prompt_template = GPT_PROMPT_CATEGORIES
    prompt = (
        prompt_template
        .replace("{{CATEGORIES}}", categories_str)
        .replace("{video_title}", video_title)
        .replace("{transcript}", final_transcript)
    )
    temperature = GPT_TEMPERATURE_CATEGORIES

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[{"role": "system", "content": prompt}],
            temperature=temperature
        )

        content = response.choices[0].message.content.strip()
        cleaned = extract_json_block(content)
        parsed = json.loads(cleaned)

        usage = response.usage
        return {
            "keywords": parsed.get("keywords", []),
            "categories": parsed.get("categories", []),
            "prompt": prompt,
            "temperature": temperature,
            "tokens_input": usage.prompt_tokens,
            "tokens_output": usage.completion_tokens
        }

    except Exception as e:
        print(f"‚ùå Error llamando a Azure OpenAI: {e}")
        return None

# -----------------------------
# TEST DE PRUEBA
# -----------------------------

def test_keywords_categories():
    video_title = "Qu√© alimentos evitar para no arruinar tu viaje al extranjero"
    final_transcript = """
    RECOMENDACIONES PARA VIAJAR AL EXTRANJERO

1. Prepara el viaje con suficiente antelaci√≥n.

2. Siempre usa agua potable para beber y preparar alimentos.

3. Mejor beber de la lata o la botella.

4. Si no tienes agua potable, es m√°s seguro usar agua muy caliente del grifo.

5. No a√±adas hielo a las bebidas.

6. Caf√©, t√©, vino, cervezas y gaseosas suelen ser seguras.

7. Sigue el principio de la OMS: "CUECELO, P√âLALO O OLVIDALO".

8. No comas alimentos crudos.

9. Evita las frutas sin pelar y las ensaladas.

10. Preferiblemente consume alimentos reci√©n cocinados.

11. Evita la leche no esterilizada y los derivados l√°cteos.

12. No comas helados si desconoces su procedencia.

¬°DISFRUTA TUS VACACIONES CON SEGURIDAD!
    """
    categories = ["ALIMENTACI√ìN SALUDABLE", "BIENESTAR MENTAL", "SALUD CARDIOVASCULAR", "EJERCICIO F√çSICO", "BIENESTAR CORPORAL", "SALUD FEMENINA", "MEDITACI√ìN Y MINDFULNESS", "PREVENCI√ìN DEL C√ÅNCER", "PRIMEROS AUXILIOS", "SALUD VISUAL", "TABAQUISMO", "PEDIATR√çA"]

    result = obtain_keywords_categories(final_transcript, video_title, categories)

    if result:
        print("\n‚úÖ Resultado completo:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("‚ùå No se pudo obtener resultado.")

# -----------------------------
# MAIN
# -----------------------------

if __name__ == "__main__":
    test_keywords_categories()
</file>

<file path="src/prueba_download.py">
import os
import re
import time
import requests
import subprocess
from typing import Dict, Union, Optional
from urllib.parse import urlparse
from datetime import datetime
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Variables globales
ACCESS_TOKEN = os.getenv('VIMEO_ACCESS_TOKEN')
HEADERS = {'Authorization': f'Bearer {ACCESS_TOKEN}'}

# Directorios de trabajo
RAW_VIDEOS_DIR = '/Users/clara/proyectos/vplay/vitaly-transcription-and-categorization/tmp/video'  # <-- Crea esta carpeta si no existe
AUDIO_DIR = '/Users/clara/proyectos/vplay/vitaly-transcription-and-categorization/tmp/audio'      # <-- Crea esta carpeta si no existe

# --------- Funciones auxiliares ---------

def sanitize_filename(name: str) -> str:
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '')
    name = name.replace(' ', '_')
    return name

def get_video_id(video_url: str) -> str:
    path = urlparse(video_url).path
    return path.strip('/')

def get_video_duration(video_url: str) -> Optional[float]:
    try:
        video_id = get_video_id(video_url)
        api_url = f"https://api.vimeo.com/videos/{video_id}"
        response = requests.get(api_url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()
        duration = data.get('duration')
        if duration is not None:
            return round(float(duration), 2)
        else:
            print("‚ö†Ô∏è  No se encontr√≥ duraci√≥n en el video.")
            return None
    except Exception as e:
        print(f"‚ùå Error obteniendo duraci√≥n del video desde Vimeo API: {e}")
        return None

def download_video(url: str, filename: str, title: str, description: str, author: str, folder: str, operation: str
                   ) -> Union[str, Dict[str, str], None]:
    try:
        video_id = get_video_id(url)
        api_url = f"https://api.vimeo.com/videos/{video_id}"
        response = requests.get(api_url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()

        if 'download' not in data or not data['download']:
            print('‚ö†Ô∏è  No hay enlaces de descarga disponibles.')
            return None

        download_options = data['download']
        best_option = max(download_options, key=lambda x: x.get('width', 0))
        download_link = best_option['link']

        safe_filename = sanitize_filename(filename) + '.mp4'
        video_path = os.path.join(folder, safe_filename)

        # Descargar video
        print(f"‚¨áÔ∏è  Descargando {safe_filename}...")
        with requests.get(download_link, stream=True) as r:
            r.raise_for_status()
            with open(video_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"‚úÖ Descarga completada: {video_path}")

        return video_path

    except Exception as e:
        print(f"‚ùå Error descargando el video {url}: {e}")
        return None

def convert_video_to_audio(video_path: str, audio_dir: str, conn=None) -> Optional[str]:
    video_filename = os.path.basename(video_path)
    base_name = os.path.splitext(video_filename)[0]
    audio_filename = f"{base_name}.wav"
    audio_path = os.path.join(audio_dir, audio_filename)

    if not os.path.exists(video_path):
        print(f"‚ùå Error: El archivo {video_filename} no existe.")
        return None

    start_time = time.time()

    try:
        subprocess.run([
            "ffmpeg", "-v", "warning", "-i", video_path, "-ac", "1", "-ar", "16000", audio_path, "-y"
        ], check=True)

        elapsed_time = round(time.time() - start_time, 2)
        print(f"üéµ Audio generado en {elapsed_time} segundos.")

        return audio_path

    except subprocess.CalledProcessError as e:
        print(f"‚ùå Error al convertir video a audio {video_filename}: {e}")
        return None

def run_download_pipeline(pubsub_data: Dict[str, str], conn=None, operation: str = "create") -> Optional[str]:
    title = pubsub_data.get("title")
    url = pubsub_data.get("url")
    author = pubsub_data.get("author", "Desconocido")
    description = pubsub_data.get("description", "")

    print(f"\nüöÄ Procesando entrada manual: {title} - {url}\n")

    if not title or not url:
        print("‚ùå Error: T√≠tulo o URL faltante en los datos.")
        return None

    filename = sanitize_filename(title)

    video_result = download_video(
        url=url,
        filename=filename,
        title=title,
        description=description,
        author=author,
        folder=RAW_VIDEOS_DIR,
        operation=operation
    )

    if video_result:
        audio_path = convert_video_to_audio(video_result, AUDIO_DIR, conn)
        print("‚úÖ Video procesado correctamente (FIN DEL PROCESO).")
        return audio_path
    else:
        print("‚ùå No se pudo descargar el video.")
        return None

# --------- Ejecuci√≥n directa ---------

if __name__ == "__main__":
    # Datos de prueba
    test_pubsub_data = {
        "title": "Mi video de prueba",
        "url": "https://vimeo.com/689345342",  # <-- Cambia aqu√≠ la URL real
        "author": "Autor de prueba",
        "description": "Video de prueba para descarga."
    }

    # Crear carpetas si no existen
    os.makedirs(RAW_VIDEOS_DIR, exist_ok=True)
    os.makedirs(AUDIO_DIR, exist_ok=True)

    run_download_pipeline(test_pubsub_data)
</file>

<file path="config/settings.json">
{
    "paths": {
        "base_dir": "<ruta_absoluta_del_proyecto>",
        "data_dir": "<base_dir>/tmp",
        "raw_videos_dir": "<base_dir>/tmp/video",
        "audio_dir": "<base_dir>/tmp/audio",
        "stems_dir": "<base_dir>/tmp/stems",
        "frames_dir": "<base_dir>/tmp/frames"
    },
    "audio": {
        "format": "wav"
    },
    "thresholds": {
        "voice": 0.5,
        "music": 0.01
    },
    "transcription": {
        "model": "medium",
        "device": "cpu"
    },
    "files": {
        "video_filename": null,
        "audio_filename": null
    },
    "frame_interval_seconds": 1,
    "ocr": {
        "languages_frames": ["es", "en"]
    },
    "gpt": {
        "temperature_frames": 0.2,
        "prompt_frames": "Limpia el siguiente texto extra√≠do de OCR, eliminando repeticiones, errores de reconocimiento y reorganizando las frases correctament. El texto final debe estar en espa√±ol. Texto:\n\n",
        "prompt_categories": "El siguiente t√≠tulo y texto forman parte del contenido de un video.\n\nT√≠tulo: \"{video_title}\"\n\nTexto transcrito:\n\"\"\"\n{transcript}\n\"\"\"\n\nTu tarea es:\n1. Extraer una lista de palabras clave (keywords) de la longitud que consideres util en cada caso que representen los temas centrales del video. No es necesario que todas aparezcan literalmente: tambi√©n puedes inferir conceptos relevantes que est√©n impl√≠citos, siempre que est√©n bien fundamentados en el contenido.\n2. Seleccionar una √∫nica categor√≠a de entre las siguientes (m√°ximo dos o tres solo si es claramente necesario): {{CATEGORIES}}\n\nDevuelve un JSON en el siguiente formato:\n{\n  \"keywords\": [\"palabra1\", \"palabra2\", \"...\"],\n  \"categories\": [\"categor√≠a1\", \"categor√≠a2\"]\n}\n\nInstrucciones:\n- Las keywords deben ser sustantivos o sintagmas nominales con significado propio. Evita conectores, adjetivos gen√©ricos o frases vac√≠as como \"dif√≠cil\" o \"c√≥mo hacerlo\".\n- Incluye tanto t√©rminos que aparezcan expl√≠citamente como otros que, aunque no est√©n nombrados, puedan deducirse razonablemente del contexto (por ejemplo, si se habla de dejar de fumar, puedes incluir \"dejar de fumar\", \"adicci√≥n\", \"nicotina\", \"adicci√≥n al tabaco\", \"salud p√∫blica\").\n- Prioriza t√©rminos t√©cnicos, tem√°ticos o relevantes para facilitar la b√∫squeda del contenido.\n- No incluyas sin√≥nimos repetidos ni t√©rminos demasiado gen√©ricos como \"tema\", \"informaci√≥n\", \"importancia\".\n- La lista de categor√≠as debe contener una sola categor√≠a en la mayor√≠a de los casos. Solo incluye una segunda o tercera si claramente se tratan temas relevantes para m√°s de una.\n- S√© preciso y riguroso, como si prepararas etiquetas para mejorar la b√∫squeda inteligente del contenido.\n",
        "temperature_categories": 0
    },
    "pubsub":{
        "PROJECT_ID": "tps-vitaly",
        "INPUT_TOPIC_ID": "topic-vplay-recommender-video-information-dev",
        "INPUT_SUBSCRIPTION_ID": "sub-vplay-recommender-video-information-dev",
        "RESPONSE_TOPIC_ID": "topic-vplay-recommender-user-recomendations-dev"
    }
}
</file>

<file path="no_pubsub/run_batch_processing.py">
import os, sys
import csv
import shutil
import sqlalchemy
from typing import List, Dict, Optional


BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
SRC_DIR = os.path.join(BASE_DIR, "src")
sys.path.append(SRC_DIR)

from download_videos import run_download_pipeline
from detect_music import run_music_detection
from classify_categories import run_classification
from utils_db import connect_db




# ==== CONFIGURACI√ìN ====
CSV_PATH = os.path.join(BASE_DIR, "tmp", "videos.csv")
PAGE = 1
PAGE_SIZE = 20
OPERATION = "create"

# ==== FUNCIONES ====

def load_csv(csv_path: str) -> List[Dict[str, str]]:
    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        return list(reader)

def clean_tmp_folders() -> None:
    """
    Elimina todos los archivos y subdirectorios de las carpetas temporales del proyecto.
    """
    tmp_folders = ["tmp/audio", "tmp/video", "tmp/stems", "tmp/frames"]
    for folder in tmp_folders:
        full_path = os.path.join(BASE_DIR, folder)
        if os.path.exists(full_path):
            for f in os.listdir(full_path):
                f_path = os.path.join(full_path, f)
                try:
                    if os.path.isfile(f_path) or os.path.islink(f_path):
                        os.remove(f_path)
                    elif os.path.isdir(f_path):
                        shutil.rmtree(f_path)
                except Exception as e:
                    print(f"No se pudo eliminar {f_path}: {e}")
    
    separated_dir = os.path.join(BASE_DIR, "separated")
    if os.path.exists(separated_dir):
        try:
            shutil.rmtree(separated_dir)
        except Exception as e:
            print(f"No se pudo eliminar la carpeta 'separated': {e}")

    print("üßπ Carpetas temporales limpias.")

def run_batch(csv_path: str, page: int, page_size: int, operation: str):
    videos = load_csv(csv_path)
    total = len(videos)
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    chunk = videos[start:end]

    print(f"üé¨ Procesando v√≠deos {start + 1} a {end} de {total}")
    conn = connect_db()

    for i, video in enumerate(chunk, start=start + 1):
        print(f"\n============== [{i}] {video['post_title']} ==============")

        pubsub_data = {
            "title": video["post_title"],
            "url": video["url"],
            "author": video.get("author", "Desconocido"),
            "description": video.get("description", "")
        }

        audio_path = run_download_pipeline(pubsub_data, conn, operation)
        if not audio_path:
            print("‚õî Fallo en descarga o conversi√≥n. Saltando al siguiente.")
            clean_tmp_folders()
            continue

        run_music_detection(audio_path, conn)
        run_classification(conn, audio_path, operation)

        # Limpieza despu√©s de procesar cada v√≠deo
        clean_tmp_folders()

    conn.close()
    print("\n‚úÖ Procesamiento por lotes finalizado.")

# ==== EJECUCI√ìN ====

if __name__ == "__main__":
    run_batch(CSV_PATH, PAGE, PAGE_SIZE, OPERATION)
</file>

<file path="README.md">
# Vplay: Transcription and categories

## Description
Vplay es un sistema dise√±ado para analizar autom√°ticamente contenido audiovisual (videos) y <u>generar keywords y categories</u> que puedan ser utilizados despu√©s para clasificarlos.

Para obtener estas keywords y ctaegories necesitamos analizar los elementos de audio y/o de imagen de cada video, seg√∫n en qu√© parte est√© la informaci√≥n m√°s relevante del video. 

En primer lugar, se registran los datos de la llamada de pub/sub con los datos del video que se quiere procesar. A partir de ellos, descargamos el video de su url de vimeo. A continuaci√≥n, para detectar los distintos casos que puede haber, hay que pasar el video a audio (formato .wav). Una vez aqu√≠ detectamos si hay voz con el modelo silero-vad y si hay musica con el modelo demucs.

- En el caso de que haya m√∫sica y no haya voz, se tratar√≠a de un caso **Instrumental**. Al no haber informaci√≥n de voz habalda, se entiende que toda la informaci√≥n √∫til est√° en la imagene, por lo que se procesa el video extrayendo un frame cada segundo. Sobre estos frames se realiza un reconocimiento de ocr con easyocr de los frames en el orden de obtenci√≥n. Todo el texto extra√≠do se almacena en un √∫nico texto en la base de datos para posteriormente limpiarlo con GPT y evitar repeticiones de texto entre frames consecutivos.

- En el caso de que no se detecte ni m√∫sica ni voz, se toma como un caso Instrumental y se analiza solo a partir de la imagen.

- En el caso de que haya voz y no haya m√∫sica se est√° en un caso de **Narraci√≥n** y se enentiende que toda la informaci√≥n relevante se puede extraer del audio. Para ello procesamos el audio con Whisper y extraemos directamente la transcripci√≥n. Con el modelo medium sale suficientemente bien redactado y no necesita una limpieza de texto.

- En el caso de que se reconozca tanto voz como m√∫sica se est√° ante un caso **Mixto**. Dentro de esta posibilidad se debe separar el audio en voz y otras pistas (utilizando demucs). La pista vocal nos sirve para reconocer en el audio qu√© cantidad de voz hablada, voz cantada o ruido hay mediante el modelo clap. 
    - Si hay m√°s de un 75% de voz hablada, se sigue el mismo procedimiento que en el caso Narraci√≥n. 
    - Si hay menos de un 75% de voz hablada, por un lado se reconocen los segmentos con voz y se aplica Whisper en ellos, y por otro se aplica ocr a los segmentos en los que no se reconoce voz hablada. 
    - Para obtener la transcripci√≥n final, se pasa a GPT ambas transcripciones junto al t√≠tulo del video, pidi√©ndole que cree un solo texto a partir de una o ambas transcripciones, en funci√≥n de las que tengan que ver con el t√≠tulo.

Por √∫ltimo, con a partir de los textos generados y los t√≠tulos correspondientes a cada video, se pide a GPT que obtenga las keywords y escoja las categories en las que mejor encaja cada caso. Estas categories est√°n en la tabla Category de la base de datos que se cargan autom√°ticamente, desde donde se pueden modificar si hace falta.

Todos los modelos utilizados en este proyectos se cargan desde local (/models) y se instalan desde la carpeta setup DESPU√âS de instalar los requirements.

## Installation
Para asegurar la correcta instalaci√≥n de los modelos es importante seguir el orden de instalaci√≥n:
1. Instalar todos los requisitos de requirements.txt.
2. Instalar los modelos ejecutando cada archivo de la carpeta setup. Se guardar√°n en la carpeta models y no es necesario ajustar nada m√°s.

## Usage
Se recibe un mensaje a trav√©s de pub/sub con los datos del video. A partir de la url se descarga el video y se inicia el proceso de clasificaci√≥n. 
En primer lugar se ejecuta el m√≥dulo download_video.py, que descarga el video y lo convierte a audio, actualizando los 

## Support
Tell people where they can go to for help. It can be any combination of an issue tracker, a chat room, an email address, etc.

## Roadmap
If you have ideas for releases in the future, it is a good idea to list them in the README.

## Contributing
State if you are open to contributions and what your requirements are for accepting them.

For people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. Make these steps explicit. These instructions could also be useful to your future self.

You can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser.

## Authors and acknowledgment
Show your appreciation to those who have contributed to the project.

## License
For open source projects, say how it is licensed.

## Project status
If you have run out of energy or time for your project, put a note at the top of the README saying that development has slowed down or stopped completely. Someone may choose to fork your project or volunteer to step in as a maintainer or owner, allowing your project to keep going. You can also make an explicit request for maintainers.
</file>

<file path="src/utils_narracion.py">
"""
Este m√≥dulo transcribe archivos de audio usando Whisper de OpenAI.
        1.	Carga la configuraci√≥n desde config.json (modelo, dispositivo y rutas).
        2.	Ejecuta la transcripci√≥n con el modelo especificado.
        3.	Devuelve el texto transcrito y el tiempo de procesamiento, mostrando avisos en la terminal.
"""

import warnings

warnings.filterwarnings("ignore", category=UserWarning)
import json
import os
import time
from typing import Tuple

import whisper

# =======================
# CONFIGURACI√ìN
# =======================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

paths = config["paths"]
WHISPER_MODEL = config["transcription"]["model"]
WHISPER_DEVICE = config["transcription"]["device"]


# =======================
# TRANSCRIPCI√ìN PRINCIPAL
# =======================


def transcribe_audio(
    file_path: str, model_name: str = WHISPER_MODEL, device: str = WHISPER_DEVICE
) -> Tuple[str, float]:
    """
    Transcribe un archivo de audio usando el modelo Whisper especificado.

    Args:
        file_path (str): Ruta del archivo de audio a transcribir.
        model_name (str): Nombre del modelo Whisper (por defecto desde config).
        device (str): Dispositivo a usar (cpu o cuda).

    Returns:
        Tuple[str, float]: Texto transcrito y tiempo de procesamiento en segundos.
    """
    start_time = time.time()

    model = whisper.load_model(model_name).to(device)
    result = model.transcribe(file_path)

    elapsed_time = round(time.time() - start_time, 2)

    return result["text"], elapsed_time
</file>

<file path="setup/setup_demucs.py">
import os, subprocess

# =======================
# RUTAS Y CONFIGURACI√ìN
# =======================

# Definir la ruta base donde se guardar√° el modelo DEMUCS
BASE_DIR = os.path.abspath("models")
DEMUC_DIR = os.path.join(BASE_DIR, "demucs")
os.makedirs(DEMUC_DIR, exist_ok=True)

# Configurar Hugging Face para almacenar modelo en `models/demucs/`
os.environ["HF_HOME"] = DEMUC_DIR

# =======================
# FUNCIONES DE INSTALACI√ìN
# =======================
def is_demucs_installed() -> bool:
    """
    Verifica si la herramienta DEMUCS est√° instalada en el sistema.

    Returns:
        bool: True si DEMUCS est√° disponible en el entorno, False si no lo est√°.
    """
    try:
        subprocess.run(["demucs", "--help"], check=True, capture_output=True, text=True)
        return True
    except FileNotFoundError:
        return False

def install_demucs() -> None:
    """
    Instala la herramienta DEMUCS usando pip si no est√° disponible.
    En caso de error durante la instalaci√≥n, muestra un mensaje por consola.
    """
    print("DEMUCS no est√° instalado. Instalando...")
    try:
        subprocess.run(["pip", "install", "demucs"], check=True)
        print("DEMUCS instalado correctamente.")
    except subprocess.CalledProcessError:
        print("‚ùå Error al instalar DEMUCS.")

def setup_demucs() -> None:
    """
    Verifica si DEMUCS est√° instalado y, si no lo est√°, lo instala autom√°ticamente.
    """
    if not is_demucs_installed():
        install_demucs()


# =======================
# EJECUCI√ìN
# =======================

if __name__ == "__main__":
    print("[SETUP_DEMUCS] Iniciando")
    setup_demucs()
    print("[SETUP_DEMUCS] Fin")
</file>

<file path="setup/setup_silero_vad.py">
import os, requests, sys

# =======================
# RUTAS Y CONFIGURACI√ìN
# =======================
BASE_DIR = os.path.abspath("models/silero-vad")
MODEL_PATH = os.path.join(BASE_DIR, "silero_vad.onnx")
UTILS_PATH = os.path.join(BASE_DIR, "utils_vad.py")
os.makedirs(BASE_DIR, exist_ok=True)

# URLs nuevas del modelo en git
MODEL_URL = "https://github.com/snakers4/silero-vad/raw/master/src/silero_vad/data/silero_vad.onnx"
UTILS_URL = "https://raw.githubusercontent.com/snakers4/silero-vad/master/src/silero_vad/utils_vad.py"

# =======================
# FUNCIONES DE DESCARGA
# =======================

def download_file(url: str, destination_path: str, binary: bool = False) -> None:
    """
    Descarga un archivo desde una URL y lo guarda localmente.

    Args:
        url (str): URL del archivo a descargar.
        destination_path (str): Ruta donde guardar el archivo descargado.
        binary (bool): Si es True, guarda en modo binario (por ejemplo para .onnx).
    """
    response = requests.get(url, stream=binary)
    if response.status_code == 200:
        mode = "wb" if binary else "w"
        encoding = None if binary else "utf-8"
        with open(destination_path, mode, encoding=encoding) as f:
            if binary:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            else:
                f.write(response.text)
        print(f"‚úÖ Descargado correctamente: {os.path.basename(destination_path)}")
    else:
        print(f"‚ùå Error al descargar {os.path.basename(destination_path)}. C√≥digo HTTP: {response.status_code}")

# =======================
# FLUJO PRINCIPAL
# =======================

def setup_silero_vad() -> None:
    """
    Descarga el modelo y las utilidades de Silero-VAD si no existen,
    y asegura que el archivo de utilidades est√° disponible en el path.
    """
    if not os.path.exists(MODEL_PATH):
        download_file(MODEL_URL, MODEL_PATH, binary=True)

    if not os.path.exists(UTILS_PATH):
        download_file(UTILS_URL, UTILS_PATH, binary=False)

    if not os.path.exists(UTILS_PATH):
        raise FileNotFoundError(f"‚ùå utils_vad.py no encontrado en: {UTILS_PATH}")

    sys.path.insert(0, BASE_DIR)
    print("‚úÖ Descarga de Silero-VAD completada.")

# =======================
# EJECUCI√ìN
# =======================

if __name__ == "__main__":
    print("[SETUP_SILERO] Iniciando")
    setup_silero_vad()
    print("[SETUP_SILERO] Fin")


# # Descargar el modelo propio
# if not os.path.exists(MODEL_PATH):
#     response = requests.get(MODEL_URL, stream=True)
#     if response.status_code == 200:
#         with open(MODEL_PATH, "wb") as f:
#             for chunk in response.iter_content(chunk_size=8192):
#                 f.write(chunk)
#     else:
#         print(f"‚ùå Error descargando el modelo. C√≥digo HTTP: {response.status_code}")

# # Descarga el utils del modelo
# if not os.path.exists(UTILS_PATH):
#     response = requests.get(UTILS_URL)
#     if response.status_code == 200:
#         with open(UTILS_PATH, "w", encoding="utf-8") as f:
#             f.write(response.text)
#     else:
#         print(f"‚ùå Error descargando funciones auxiliares. C√≥digo HTTP: {response.status_code}")

# # Agregar la ruta de utils_vad.py al sys.path manualmente
# if not os.path.exists(UTILS_PATH):
#     raise FileNotFoundError(f"‚ùå utils_vad.py no encontrado en: {UTILS_PATH}")
# sys.path.insert(0, BASE_DIR)

# print("Descarga de Silero-vad completada.")
</file>

<file path="src/utils_error.py">
from typing import Optional, Union

def format_success(message: str, url: Optional[str] = None) -> dict:
    """
    Retorna una respuesta con formato de √©xito est√°ndar.

    Args:
        message (str): Mensaje descriptivo de √©xito.
        url (str, optional): URL asociada al recurso procesado.

    Returns:
        dict: Diccionario con estado 'success' y detalles.
    """
    return {
        "status": "success",
        "message": message,
        "url": url,
        "error_code": None
    }

def format_error(message: str, error_code: str, url: Optional[str] = None, details: Optional[Union[str, dict]] = None) -> dict:
    """
    Retorna una respuesta con formato de error est√°ndar.

    Args:
        message (str): Descripci√≥n del error.
        error_code (str): C√≥digo interno del tipo de error.
        url (str, optional): URL del video o recurso asociado.
        details (str | dict, optional): Informaci√≥n adicional.

    Returns:
        dict: Diccionario con estado 'error' y detalles.
    """
    return {
        "status": "error",
        "message": message,
        "url": url,
        "error_code": error_code,
        "details": details
    }

# =======================
# =======================
# =======================
# Todos estos devuelven un diccionario con el formato de format_error().

def error_invalid_message_format(details: Optional[str] = None) -> dict:
    return format_error(
        message="El mensaje recibido tiene un formato incorrecto o faltan campos obligatorios.",
        error_code="INVALID_MESSAGE_FORMAT",
        details=details
    )

def error_video_id_exists(video_id: str) -> dict:
    return format_error(
        message=f"El video con ID '{video_id}' ya existe y no se puede insertar de nuevo.",
        error_code="VIDEO_ID_EXISTS"
    )

def error_video_id_not_found(video_id: str) -> dict:  # accedido desde utils_db.py -> insert_video_metadata
    return format_error(
        message=f"El video con ID '{video_id}' no existe y no se puede hacer crear.",
        error_code="VIDEO_ID_TO_MODIFY_NOT_FOUND"
    )

def error_video_id_to_modify_not_found(video_id: str) -> dict:  # accedido desde utils_db.py -> insert_video_metadata
    return format_error(
        message=f"El video con ID '{video_id}' no existe y no se puede modificar.",
        error_code="VIDEO_ID_TO_MODIFY_NOT_FOUND"
    )

def error_video_id_to_delete_not_found(video_id: str) -> dict:  # accedido desde utils_db.py -> insert_video_metadata
    return format_error(
        message=f"El video con ID '{video_id}' no existe y no se puede borrar.",
        error_code="VIDEO_ID_TO_DELETE_NOT_FOUND"
    )

def pipeline_aborted_by_deletion(video_id: str) -> dict:
    return format_error(
        message=f"El video con ID '{video_id}' fue eliminado. No hay m√°s procesamiento que realizar.",
        error_code="PIPELINE_ABORTED_BY_DELETION"
    )

def error_video_url_not_accessible(url: str) -> dict:
    return format_error(
        message=f"No se pudo acceder a la URL del video: {url}",
        error_code="VIDEO_URL_UNREACHABLE",
        url=url
    )

def error_database_connection(details: Optional[str] = None) -> dict:
    return format_error(
        message="No se pudo establecer conexi√≥n con la base de datos.",
        error_code="DATABASE_CONNECTION_ERROR",
        details=details
    )

def error_pipeline_failed(url: Optional[str] = None, details: Optional[str] = None) -> dict:
    return format_error(
        message="El procesamiento del video fall√≥.",
        error_code="PIPELINE_FAILED",
        url=url,
        details=details
    )
</file>

<file path="src/utils_mixto.py">
import os, sys, subprocess, json
import torch, torchaudio
from openai import AzureOpenAI
from dotenv import load_dotenv
from torchaudio.transforms import Resample
from transformers import AutoProcessor, AutoModel
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.utils_narracion import transcribe_audio

# =======================
# CONFIGURACI√ìN AZURE
# =======================
load_dotenv()
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Checkear variables
if not all([endpoint, deployment, api_key, api_version]):
    raise ValueError("‚ùå Error al cargar variables de entorno para Azure OpenAI.")

# Iniciar cliente de Azure OpenAI
client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version=api_version
)

# =======================
# CONFIGURACI√ìN DIRECTORIOS Y UMBRALES
# =======================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
MODEL_DIR = os.path.join(BASE_DIR, "models")
DATA_DIR = os.path.join(BASE_DIR, "tmp")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
FRAMES_DIR = os.path.join(DATA_DIR, "frames")

CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

# Umbrales desde `config.json`
gpt_config = config["gpt"]
GPT_TEMPERATURE_FRAMES = gpt_config["temperature_frames"]
GPT_PROMPT_FRAMES = gpt_config["prompt_frames"]

# =======================
# AUXILIARES - GENERALES DE PROCESS_VOCALS_WITH_CLAP
# =======================

def load_clap_model() -> tuple[AutoProcessor, AutoModel] | tuple[None, None]:
    """
    Carga el modelo y procesador CLAP desde una ruta local.

    Returns:
        tuple: (processor, model) si se carg√≥ correctamente, o (None, None) si falla.
    """
    model_path = os.path.join(BASE_DIR, "models/clap-htsat-unfused")

    if not os.path.exists(model_path):
        print(f"‚ùå Error: No se encontr√≥ el modelo en {model_path}")
        return None, None

    processor = AutoProcessor.from_pretrained(model_path)
    model = AutoModel.from_pretrained(model_path).eval()

    return processor, model

def prepare_audio_for_clap(audio_path: str, target_sample_rate: int = 48000) -> tuple[torch.Tensor, int]:
    """
    Carga y prepara un archivo de audio para ser procesado con CLAP.

    Args:
        audio_path (str): Ruta del archivo de audio.
        target_sample_rate (int): Frecuencia de muestreo deseada.

    Returns:
        tuple: (tensor de audio mono, sample_rate ajustado)
    """
    wav_data, sample_rate = torchaudio.load(audio_path)
    wav_data = wav_data.mean(dim=0)  # Convertir a mono

    if sample_rate != target_sample_rate:
        resampler = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)
        wav_data = resampler(wav_data)
        sample_rate = target_sample_rate

    return wav_data, sample_rate


def classify_audio_with_clap(wav_data: torch.Tensor, sample_rate: int, processor, model) -> tuple[float, list[tuple[int, int, torch.Tensor]]]:
    """
    Clasifica segmentos de audio en categor√≠as usando CLAP, identificando partes habladas.

    Args:
        wav_data (Tensor): Audio mono como tensor.
        sample_rate (int): Frecuencia de muestreo.
        processor (AutoProcessor): Procesador CLAP.
        model (AutoModel): Modelo CLAP.

    Returns:
        tuple: (speaking_ratio, lista de segmentos hablados)
    """
    candidates = ["Speaking", "Singing", "Silence", "Noise", "Other"]

    # Configuraci√≥n para el procesamiento por chunks de audio
    chunk_size = 3
    overlap = 1
    chunk_samples = int(sample_rate * chunk_size)
    step = chunk_samples - int(sample_rate * overlap)

    # Variables para an√°lisis del contenido del audio
    speaking_count = 0
    total_chunks = 0
    speaking_segments = []

    for start in range(0, len(wav_data), step):
        end = min(start + chunk_samples, len(wav_data))
        chunk = wav_data[start:end]

        # Preprocesar audio y texto para CLAP
        inputs_audio = processor(audios=chunk.numpy(), return_tensors="pt", sampling_rate=sample_rate)
        inputs_text = processor(text=candidates, return_tensors="pt", padding=True)

        # Extraer embeddings y calcular similitudes
        with torch.no_grad():
            audio_embedding = model.get_audio_features(**inputs_audio)
            text_embedding = model.get_text_features(**inputs_text)

        similarities = torch.nn.functional.cosine_similarity(audio_embedding, text_embedding, dim=-1)
        best_match = candidates[torch.argmax(similarities).item()]

        if best_match == "Speaking":
            speaking_count += 1
            speaking_segments.append((start, end, chunk))

        total_chunks += 1
        if end == len(wav_data):
            break

    speaking_ratio = speaking_count / total_chunks if total_chunks > 0 else 0
    print("SPEAKING RATIO:", round(speaking_ratio, 2))
    return speaking_ratio, speaking_segments

# =======================
# AUXILIARES <75% - PROCESS_VOCALS_WITH_CLAP <75%
# =======================

def merge_speaking_segments(speaking_segments: list[tuple[int, int, torch.Tensor]], sample_rate: int, chunk_size: int = 3, overlap: int = 1) -> torch.Tensor | None:
    """
    Une los segmentos 'Speaking' en un solo audio sin repeticiones por solapamiento.

    Returns:
        Tensor: Audio combinado, o None si no hay segmentos.
    """
    if not speaking_segments:
        return None  # No hay segmentos que fusionar

    speaking_segments.sort(key=lambda x: x[0]) 

    merged_audio = []
    prev_end = None  # Mantendr√° el final del √∫ltimo segmento procesado

    non_overlap_samples = int(sample_rate * (chunk_size - overlap))  # Tama√±o sin overlap
    initial_segment_samples = int(sample_rate * (chunk_size - (overlap / 2)))  # Primer segmento sin la √∫ltima mitad del overlap

    for i, (start, end, segment_audio) in enumerate(speaking_segments):
        if i == 0:
            # Para el primer segmento, tomamos los primeros 2 segundos (si overlap = 1 en chunk de 3s)
            segment_audio = segment_audio[:initial_segment_samples]
        elif prev_end is not None and start == prev_end:
            # Si el segmento es consecutivo, descartamos la parte inicial solapada
            segment_audio = segment_audio[-non_overlap_samples:]
        else:
            # Si no es consecutivo, mantenemos el segmento completo
            segment_audio = segment_audio

        merged_audio.append(segment_audio)
        prev_end = end  # Actualizamos el final del segmento procesado

    if merged_audio:
        return torch.cat(merged_audio, dim=0)  # Unimos los segmentos en un solo tensor

    return None

def transcribe_speaking_audio(merged_audio: torch.Tensor, video_id: int, sample_rate: int) -> str | None:
    """
    Transcribe el audio combinado de segmentos 'Speaking' usando Whisper.

    Returns:
        str | None: Texto transcrito o None si no hay audio.
    """
    if merged_audio is None or len(merged_audio) == 0:
        print("No se encontraron segmentos 'Speaking'. No se realizar√° transcripci√≥n.")
        return

    temp_audio_path = f"temp_{video_id}.wav"
    torchaudio.save(temp_audio_path, merged_audio.unsqueeze(0), sample_rate)

    transcription = transcribe_audio(temp_audio_path)
    os.remove(temp_audio_path)

    return transcription

def get_non_speaking_segments(total_duration: int, speaking_segments: list[tuple[int, int]], overlap: int = 1) -> list[tuple[int, int]]:
    """
    Calcula los fragmentos del audio donde no hay voz.

    Returns:
        list: Lista de tuplas (inicio, fin) de segmentos no hablados.
    """
    non_speaking_segments = []
    last_end = 0  # Marca d√≥nde termina el √∫ltimo segmento de "Speaking"

    for start, end in speaking_segments:
        adjusted_start = max(0, start - overlap)  # Restar el solapamiento sin ir m√°s all√° de 0

        if last_end < adjusted_start:
            # Hay un hueco entre el final del √∫ltimo segmento y el inicio del siguiente (ajustado)
            non_speaking_segments.append((last_end, adjusted_start))
        
        last_end = end  # Actualizar el final del √∫ltimo segmento de "Speaking"

    # Si hay un espacio despu√©s del √∫ltimo segmento "Speaking" hasta el final del video
    if last_end < total_duration:
        non_speaking_segments.append((last_end, total_duration))

    return non_speaking_segments

# =======================
# TRANSCRIPT FINAL MIXTO CON T√çTULO GPT
# =======================

def generate_mixto_summary(video_title: str, transcript_audio: str, transcript_image: str) -> tuple[str | None, int, int]:
    """
    Genera un resumen inteligente combinando el t√≠tulo del video con transcripciones de audio e imagen.

    Args:
        video_title (str): T√≠tulo original del video.
        transcript_audio (str): Transcripci√≥n del audio.
        transcript_image (str): Transcripci√≥n OCR.

    Returns:
        tuple: (resumen generado, tokens_prompt, tokens_completion) o (None, 0, 0)
    """
    if not video_title:
        print(f"‚ùå No se proporcion√≥ t√≠tulo del video. Abortando resumen.")
        return None, 0, 0

    print("\nüß™ Diagn√≥stico de inputs para pruebas:")
    print(f"transcript_audio (primeros 200 chars): {transcript_audio[:200] if transcript_audio else 'VAC√çO'}")
    print(f"transcript_image (primeros 200 chars): {transcript_image[:200] if transcript_image else 'VAC√çO'}")

    prompt_content = f"""
    Tienes el t√≠tulo de un video y dos transcripciones (una de audio y otra de imagen extra√≠da con OCR). 
    Tu tarea es comparar el contenido del t√≠tulo con ambas transcripciones y decidir cu√°l refleja mejor el tema del video. 
    Eval√∫a si alguna transcripci√≥n aporta m√°s informaci√≥n o contexto que el t√≠tulo por s√≠ solo.

    T√≠tulo del video: {video_title} 

    {f"Transcripci√≥n de audio:\n{transcript_audio}" if transcript_audio else ""}

    {f"\nTranscripci√≥n de imagen:\n{transcript_image}" if transcript_image else ""}

    **Instrucciones para responder:**
    - Si solo una de las transcripciones est√° relacionada con el t√≠tulo, devuelve solo ese texto.
    - Si ambas transcripciones est√°n estrechamente relacionadas con el t√≠tulo, genera una versi√≥n unificada.
    - Si ninguna de las transcripciones est√° estrechamente relacionada, devuelve √∫nicamente el t√≠tulo del video.
    No des explicaiones ni introducciones, contesta solo como te he dicho.
    """

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[{"role": "user", "content": prompt_content}],
            temperature=0.5
        )
        summary_text = response.choices[0].message.content.strip()
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.completion_tokens

        print("\n Transcripci√≥n final en mixto generada correctamente.")
        return summary_text, prompt_tokens, completion_tokens

    except Exception as e:
        print(f"‚ùå Error generando resumen con GPT: {e}")
        return None, 0, 0
</file>

<file path="src/utils_ocr.py">
"""
Este m√≥dulo procesa frames de videos para extraer texto con OCR y limpiarlo con GPT.
	1.	Carga la configuraci√≥n desde config.json y las credenciales de Azure OpenAI.
	2.	Obtiene la ruta del video desde la base de datos y extrae frames cada cierto intervalo.
	3.	Aplica OCR con EasyOCR para detectar texto en los frames.
	4.	Guarda las transcripciones OCR en la base de datos.
	5.	Limpia el texto con Azure OpenAI y actualiza la transcripci√≥n en la base de datos.
    """

import os, json, re
import cv2
import easyocr
from typing import Optional, Tuple
from openai import AzureOpenAI
from dotenv import load_dotenv


# =======================
# CONFIGURACI√ìN AZURE
# =======================
load_dotenv()
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Checkear variables
if not all([endpoint, deployment, api_key, api_version]):
    raise ValueError("‚ùå Error al cargar variables de entorno para Azure OpenAI.")

# Iniciar cliente de Azure OpenAI
client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version=api_version
)
    
# =======================
# RUTAS
# =======================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(BASE_DIR, "tmp")
VIDEO_DIR = os.path.join(DATA_DIR, "video")
FRAMES_DIR = os.path.join(DATA_DIR, "frames")

CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)



# ====================
# FRAME EXTRACTION
# ====================

def save_frame(video_id: int, video_path: str, frames_dir: str, interval_seconds: int = 1, non_speaking_segments: Optional[list[Tuple[float, float]]] = None) -> None:
    """
    Extrae y guarda frames de un video cada X segundos.

    Args:
        video_id (int): ID del video.
        video_path (str): Ruta al archivo de video.
        frames_dir (str): Carpeta donde se guardar√°n los frames.
        interval_seconds (int): Intervalo entre frames (en segundos).
        non_speaking_segments (Optional[list]): Lista de segmentos silenciosos para filtrar (caso Mixto).
    """
    
    if not video_path:
        print(f"‚ùå No se pudo obtener la ruta del video para video_id {video_id}")
        return

    print(f"Extrayendo frames de: {os.path.basename(video_path)}")
    video_filename = os.path.splitext(os.path.basename(video_path))[0]
    output_folder = os.path.join(frames_dir, video_filename)
    os.makedirs(output_folder, exist_ok=True)

    cap = cv2.VideoCapture(video_path)
    saved_count = 0

    if not cap.isOpened():
        print(f"‚ùå No se pudo abrir el archivo de video: {video_path}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps <= 0:
        print(f"‚ùå FPS no v√°lido para {video_path}. Abortando extracci√≥n.")
        cap.release()
        return

    frame_interval = int(fps * interval_seconds)
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_time = frame_count / fps  # Convertir frame count a segundos

        # Verificar si estamos dentro de un segmento no-speaking (solo en caso Mixto)
        if non_speaking_segments:
            in_valid_segment = any(start <= frame_time <= end for start, end in non_speaking_segments)
        else:
            in_valid_segment = True  # En el caso Instrumental, se extrae de todo el video

        if in_valid_segment and frame_count % frame_interval == 0:
            frame_filename = os.path.join(output_folder, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_filename, frame)
            saved_count += 1

        frame_count += 1

    cap.release()
    print(f"{saved_count} frames extra√≠dos.")


# ====================
# OCR
# ====================

# def update_ocr_transcripts(video_id, ocr_text, conn):
#     """Guarda la transcripci√≥n OCR en la base de datos."""
#     print(f"üìù Guardando OCR para video_id {video_id}")
#     try:
#         with conn.cursor() as cursor:
#             cursor.execute("""
#                 UPDATE transcripts
#                 SET transcript_raw_image = %s
#                 WHERE video_id = %s;                           
#             """, (ocr_text, video_id))
#         conn.commit()
#         print("Transcripci√≥n OCR guardada.")
#     except Exception as e:
#         print(f"‚ùå Error insertando OCR en la BD: {e}")


def procesar_frames_de_video(video_id: int, video_path: str, frames_dir: str, ocr_languages: list[str] = ["es", "en"]) -> Optional[str]:
    """
    Procesa los frames del video y devuelve el texto detectado mediante OCR.

    Args:
        video_id (int): ID del video.
        video_path (str): Ruta del video.
        frames_dir (str): Carpeta con los frames extra√≠dos.
        ocr_languages (list[str]): Idiomas del OCR.

    Returns:
        Optional[str]: Texto detectado o None si no se detect√≥ nada.
    """
    print("Procesando frames con ocr")
    if not video_path:
        print(f"‚ùå No se pudo obtener la ruta del video para video_id {video_id}")
        return None

    video_name = os.path.splitext(os.path.basename(video_path))[0]
    video_frames_dir = os.path.join(frames_dir, video_name)

    if not os.path.exists(video_frames_dir):
        print(f"No se encontraron frames para el video: {video_name}")
        return None

    reader = easyocr.Reader(ocr_languages)

    def obtener_numero(nombre):
        match = re.search(r'(\d+)', nombre)
        return int(match.group(1)) if match else 0

    archivos = sorted(
        [f for f in os.listdir(video_frames_dir) if f.endswith((".jpg", ".png"))],
        key=obtener_numero
    )

    textos_detectados = []

    for filename in archivos:
        image_path = os.path.join(video_frames_dir, filename)
        result = reader.readtext(image_path)
        texto_frame = "\n".join([r[1] for r in result])
        if texto_frame.strip():
            textos_detectados.append(texto_frame)

    if textos_detectados:
        texto_final = "\n\n".join(textos_detectados)
        return texto_final

    return None  # No se detect√≥ texto


# ====================
# GPT CLEANING
# ====================

def limpiar_con_gpt(texto: str, gpt_prompt: str, gpt_temperature: float = 0.7) -> Tuple[str, int, int]:
    """
    Limpia el texto OCR usando Azure OpenAI.

    Args:
        texto (str): Texto OCR original.
        gpt_prompt (str): Prompt para limpieza.
        gpt_temperature (float): Nivel de aleatoriedad.

    Returns:
        Tuple[str, int, int]: Texto limpio, tokens de entrada y salida.
    """

    response = client.chat.completions.create(
        model=deployment,
        messages=[{"role": "user", "content": gpt_prompt + "\n\n" + texto}],
        temperature=gpt_temperature
    )
    cleaned_text = response.choices[0].message.content.strip()
    return cleaned_text, response.usage.prompt_tokens, response.usage.completion_tokens


def procesar_texto(video_id: int, texto_ocr: str, gpt_prompt: str, gpt_temperature: float) -> Optional[str]:
    """
    Aplica limpieza GPT a una transcripci√≥n OCR y devuelve el resultado.

    Args:
        video_id (int): ID del video.
        texto_ocr (str): Texto OCR a limpiar.
        gpt_prompt (str): Prompt para GPT.
        gpt_temperature (float): Temperatura GPT.

    Returns:
        Optional[str]: Texto limpio o None si no hay texto.
    """
    if not texto_ocr or not texto_ocr.strip():
        print(f"No hay texto OCR sucio para limpiar en video_id {video_id}.")
        return None
    
    print(f"\nü§ñ Aplicando limpieza GPT para video_id {video_id}")

    texto_limpio, tokens_in, tokens_out = limpiar_con_gpt(texto_ocr, gpt_prompt, gpt_temperature)
    return texto_limpio
</file>

<file path="setup/setup_clap.py">
import os, shutil
from transformers import AutoModel, AutoProcessor

# =======================
# RUTAS Y CONFIGURACI√ìN
# =======================
BASE_DIR = os.path.abspath("models")
MODEL_DIR = os.path.join(BASE_DIR, 'clap-htsat-unfused')
os.makedirs(MODEL_DIR, exist_ok=True)

# =======================
# FUNCIONES DE DESCARGA
# =======================

def is_clap_downloaded() -> bool:
    """
    Verifica si el modelo CLAP ha sido descargado correctamente carg√°ndolo desde disco.
    Returns:
        bool: True si el modelo y el procesador se pueden cargar desde disco, False si falta algo o hay error.
    """
    print("Comprobando si CLAP ya est√° instalado...")

    # Si la carpeta no existe, claramente no est√° instalado
    if not os.path.isdir(MODEL_DIR):
        print("La carpeta del modelo no existe. CLAP no est√° instalado.")
        return False

    # Si existe pero est√° vac√≠a
    if not os.listdir(MODEL_DIR):
        print("La carpeta del modelo est√° vac√≠a. CLAP no est√° instalado.")
        return False

    # Si hay algo, intentamos cargar
    try:
        _ = AutoModel.from_pretrained(MODEL_DIR)
        _ = AutoProcessor.from_pretrained(MODEL_DIR)
        print("CLAP detectado en disco.")
        return True

    except OSError as e:
        print(f"Instalaci√≥n incompleta o corrupta: {e}")
        print("Eliminando carpeta incompleta...")
        shutil.rmtree(MODEL_DIR, ignore_errors=True)
        return False

    except Exception as e:
        print(f"CLAP no est√° bien instalado: {e}")
        return False

def download_clap() -> None:
    """
    Descarga el modelo CLAP y su procesador desde Hugging Face y los guarda localmente.
    Si ocurre un error durante la descarga, imprime un mensaje de error.
    """
    try:
        model = AutoModel.from_pretrained("laion/clap-htsat-unfused")
        processor = AutoProcessor.from_pretrained("laion/clap-htsat-unfused")

        model.save_pretrained(MODEL_DIR)
        processor.save_pretrained(MODEL_DIR)
        print("CLAP instalado correctamente.")
    except Exception:
        print("‚ùå Error al descargar CLAP.")

def setup_clap() -> None:
    """
    Comprueba si el modelo CLAP est√° descargado y, si no lo est√°, lo descarga.
    """
    if not is_clap_downloaded():
        print("Instalando.")
        download_clap()
    else:
        print("Omitiendo instalaci√≥n.")


# =======================
# EJECUCI√ìN
# =======================

if __name__ == "__main__":
    print("[SETUP_CLAP] Iniciando")
    setup_clap()
    print("[SETUP_CLAP] Fin")
</file>

<file path="src/config_loader.py">
import os
import json
from typing import Any, Dict

def load_settings() -> Dict[str, Any]:
    """
    Carga la configuraci√≥n desde un archivo JSON ubicado en el directorio 'config'.

    El archivo de configuraci√≥n se espera que est√© en la ruta:
    <directorio_base>/config/settings.json, donde <directorio_base> es el
    directorio padre del archivo que contiene esta funci√≥n.

    Returns:
        Dict[str, Any]: Un diccionario con las configuraciones cargadas desde el archivo JSON.
    
    Raises:
        FileNotFoundError: Si el archivo 'settings.json' no existe.
        json.JSONDecodeError: Si el contenido del archivo no es un JSON v√°lido.
    """
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    config_path = os.path.join(base_dir, 'config', 'settings.json')

    with open(config_path, 'r', encoding='utf-8') as f:
        settings: Dict[str, Any] = json.load(f)

    return settings
</file>

<file path="src/detect_music.py">
"""
1.	Clasifica el audio en tres tipos:
    ‚Ä¢	Narraci√≥n: Solo voz, sin m√∫sica.
    ‚Ä¢	Instrumental: Solo m√∫sica, sin voz.
    ‚Ä¢	Mixto: Contiene ambos.
2.	Detecta voz con Silero VAD y m√∫sica con Demucs.
3.	Actualiza la base de datos con los resultados de detecci√≥n y la clasificaci√≥n del video.
4.	Transcribe o extrae texto seg√∫n el tipo de video:
    ‚Ä¢	Narraci√≥n: Usa Whisper para transcribir.
    ‚Ä¢	Instrumental: Extrae frames, aplica OCR con EasyOCR, limpia el texto con GPT y lo guarda en la base de datos.
    ‚Ä¢	Mixto: Extrae la pista vocal con Demucs, clasifica segmentos con CLAP.
           - Si m√°s del 75% del audio es ‚ÄúSpeaking‚Äù, usa Whisper para transcribir.
            - Si es menor, ejecuta Whisper en segmentos espec√≠ficos y OCR sobre frames del video.
5.	Registra tiempos de procesamiento y almacena los resultados en la base de datos.
"""

import json
import os
import subprocess
import sys
import time

import torch
import torchaudio
import transformers as T

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from src.utils_db import (
    connect_db,
    get_text_from_ocr,
    get_transcripts_mixto_puro,
    get_video_duration,
    get_video_info,
    get_video_path,
    update_clean_ocr_transcripts,
    update_ocr_transcripts,
    update_transcripts_mixto_puro,
    update_transcripts_mixto_speaking,
    update_transcripts_narracion,
    update_video_audio_analysis,
)
from src.utils_mixto import (
    classify_audio_with_clap,
    generate_mixto_summary,
    get_non_speaking_segments,
    load_clap_model,
    merge_speaking_segments,
    prepare_audio_for_clap,
    transcribe_speaking_audio,
)
from src.utils_narracion import transcribe_audio
from src.utils_ocr import procesar_frames_de_video, procesar_texto, save_frame

sys.path.append(
    os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..", "models", "silero-vad")
    )
)
from silero_vad.model import load_silero_vad
from utils_vad import get_speech_timestamps

# =======================
# CONFIGURACI√ìN DIRECTORIOS Y UMBRALES
# =======================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
MODEL_DIR = os.path.join(BASE_DIR, "models")
DATA_DIR = os.path.join(BASE_DIR, "tmp")
VIDEO_DIR = os.path.join(DATA_DIR, "video")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
FRAMES_DIR = os.path.join(DATA_DIR, "frames")
DEMUCS_DIR = os.path.join(MODEL_DIR, "demucs")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(FRAMES_DIR, exist_ok=True)


CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

# Obtener umbrales desde `config.json`
VOICE_THRESHOLD = config["thresholds"]["voice"]
MUSIC_THRESHOLD = config["thresholds"]["music"]
WHISPER_MODEL = config["transcription"]["model"]
WHISPER_DEVICE = config["transcription"]["device"]

gpt_config = config["gpt"]
GPT_TEMPERATURE_FRAMES = gpt_config["temperature_frames"]
GPT_PROMPT_FRAMES = gpt_config["prompt_frames"]
GPT_PROMPT_CATEGORIES = gpt_config["prompt_categories"]
GPT_TEMPERATURE_CATEGORIES = gpt_config["temperature_categories"]

# Rutas de silero
SILERO_MODEL_DIR = os.path.join(MODEL_DIR, "silero-vad")
SILERO_MODEL_PATH = os.path.join(SILERO_MODEL_DIR, "silero_vad.onnx")
SILERO_UTILS_PATH = os.path.join(SILERO_MODEL_DIR, "utils_vad.py")
# Agregar la ruta de utils_vad.py al sys.path manualmente
if not os.path.exists(SILERO_UTILS_PATH):
    raise FileNotFoundError(f"‚ùå utils_vad.py no encontrado en: {SILERO_UTILS_PATH}")
sys.path.insert(0, BASE_DIR)


# =======================
# DETECCI√ìN DE VOZ Y M√öSICA
# =======================


def detect_voice(audio_path, threshold=MUSIC_THRESHOLD):
    """Detecta la presencia de voz usando Silero VAD y mide el tiempo de ejecuci√≥n.
    Convierte la tasa de muestreo a 16000 Hz si es necesario.
    """
    start_time = time.time()

    # Cargar modelo y utilidades
    if not os.path.exists(SILERO_MODEL_PATH):
        raise FileNotFoundError(
            f"‚ùå Modelo Silero no encontrado en: {SILERO_MODEL_PATH}"
        )

    model = load_silero_vad(onnx=True)

    # Cargar audio
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"‚ùå Archivo de audio no encontrado: {audio_path}")

    wav, sr = torchaudio.load(audio_path)
    wav = wav.mean(dim=0)  # Convertir a mono si es est√©reo

    # Convertir a 16000 Hz si hace falta
    target_sr = 16000
    if sr != target_sr:
        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)
        wav = resampler(wav)
        sr = target_sr  # Actualizar la tasa de muestreo

    # Detectar voz con el umbral especificado
    speech_timestamps = get_speech_timestamps(
        wav, model, sampling_rate=sr, threshold=threshold
    )
    has_voice = len(speech_timestamps) > 0

    voice_time = time.time() - start_time
    print(
        f"Detecci√≥n de voz finalizada en {round(voice_time, 2)} segundos. {'Voz detectada' if has_voice else 'No se detect√≥ voz'}"
    )

    return has_voice, voice_time, threshold


def detect_music(audio_path, threshold=MUSIC_THRESHOLD):
    """
    Detecta la presencia de m√∫sica usando DEMUCS desde l√≠nea de comandos.
    Calcula la amplitud promedio de las pistas drums, bass y other.
    """
    start_time = time.time()

    try:
        output_dir = os.path.join(BASE_DIR, "models", "demucs", "separated")
        subprocess.run(
            ["demucs", "--out", output_dir, audio_path],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Error al ejecutar DEMUCS: {e}")
        return False

    # Obtener ruta del directorio de salida
    base_name = os.path.basename(audio_path)
    name_without_ext = os.path.splitext(base_name)[0]
    demucs_output_dir = os.path.join(
        DEMUCS_DIR, "separated", "htdemucs", name_without_ext
    )

    if not os.path.exists(demucs_output_dir):
        print(f"‚ùå No se encontr√≥ la carpeta de salida de DEMUCS: {demucs_output_dir}")
        return False

    # Confirmar si vocals.wav existe (opcional, ya que no se mueve)
    vocals_path = os.path.join(demucs_output_dir, "vocals.wav")
    if os.path.exists(vocals_path):
        print(f"vocals.wav disponible en: {vocals_path}")
    else:
        print(f"vocals.wav no encontrado en {demucs_output_dir}")

    # Calcular amplitud media de las pistas no vocales
    music_stems = ["drums.wav", "bass.wav", "other.wav"]
    total_amplitude = 0
    valid_stems = 0

    for stem in music_stems:
        stem_path = os.path.join(demucs_output_dir, stem)
        if os.path.exists(stem_path):
            try:
                wav, sr = torchaudio.load(stem_path)
                amplitude = wav.abs().mean().item()
                total_amplitude += amplitude
                valid_stems += 1
            except Exception as e:
                print(f"Error al cargar {stem}: {e}")
        else:
            print(f"No se encontr√≥ {stem_path}")

    if valid_stems == 0:
        print("‚ùå No se pudieron cargar pistas musicales para calcular la amplitud.")
        return False

    avg_amplitude = total_amplitude / valid_stems
    has_music = avg_amplitude > threshold

    music_time = time.time() - start_time
    print(
        f"Amplitud promedio (sin vocals): {round(avg_amplitude, 5)} (umbral: {threshold})"
    )
    print(f"Resultado: {'M√∫sica detectada' if has_music else 'No se detect√≥ m√∫sica'}")

    return has_music


# =======================
# FUNCI√ìN PRINCIPAL
# =======================


def run_music_detection(audio_path, conn):
    """Ejecuta la detecci√≥n de voz, m√∫sica y transcripci√≥n (con conexi√≥n √∫nica a BD)."""

    try:
        filename = os.path.basename(audio_path)

        print(f"\n Analizando audio para detectar voz y/o m√∫sica: {filename}")
        has_voice, _, _ = detect_voice(audio_path)
        has_music = detect_music(audio_path)

        print("Asignando tipo de video en funci√≥n de la presencia de voz y m√∫sica")
        update_video_audio_analysis(filename, has_voice, has_music, conn)

        video_id, video_type = get_video_info(filename, conn)
        if video_id is None:
            print(
                "‚ùå No se pudo obtener video_id de la BD, comprobar la inserci√≥n inicial. Cerrando la conexi√≥n con la base de datos en esta ejecuci√≥n."
            )
            conn.close()
            return

        vocals_file = f"tmp/stems/{os.path.splitext(filename)[0]}_vocals.wav"

        # === Caso 1: Narraci√≥n ===
        if video_type == "Narraci√≥n":
            print(f"Video narrativo: transcribiendo con Whisper ({WHISPER_MODEL})...")
            transcription, elapsed_time = transcribe_audio(audio_path)

            if not transcription.strip():  # Si la transcripci√≥n est√° vac√≠a
                print(
                    f"Whisper no gener√≥ transcripci√≥n para {audio_path}. Se guardar√° '-' en la BD."
                )
                transcription = "-"
                elapsed_time = 0

            update_transcripts_narracion(
                video_id, WHISPER_MODEL, elapsed_time, transcription, conn
            )

        # === Caso 2: Instrumental ===
        elif video_type in ["Instrumental", "No_categorizable"]:
            print(f"Video instrumental: extrayendo frames y aplicando OCR...")
            start_time = time.time()

            video_path = get_video_path(video_id, VIDEO_DIR, conn)
            save_frame(video_id, video_path, FRAMES_DIR, conn)

            ocr_text = procesar_frames_de_video(video_id, video_path, FRAMES_DIR)
            if ocr_text:
                update_ocr_transcripts(video_id, ocr_text, conn)

            texto_ocr = get_text_from_ocr(video_id, conn)
            cleaned_text = procesar_texto(
                video_id, texto_ocr, GPT_PROMPT_FRAMES, GPT_TEMPERATURE_FRAMES, conn
            )
            processing_time = round(time.time() - start_time, 2)
            if cleaned_text:
                update_clean_ocr_transcripts(
                    video_id, cleaned_text, processing_time, conn
                )

        # === Caso 3: Mixto
        elif video_type == "Mixto":
            print(f"Video mixto: separando speaking con CLAP...")
            vocals_file = os.path.join(
                DEMUCS_DIR,
                "separated",
                "htdemucs",
                os.path.splitext(os.path.basename(audio_path))[0],
                "vocals.wav",
            )

            if not os.path.exists(vocals_file):
                print(f"‚ùå vocals.wav no encontrado en: {vocals_file}")
                return

            # Carga el modelo CLAP
            processor, model = load_clap_model()
            if processor is None or model is None:
                return None

            # Carga y ajusta el audio
            wav_data, sample_rate = prepare_audio_for_clap(audio_path)

            # Clasifica los fragmentos de audio con CLAP
            speaking_ratio, speaking_segments = classify_audio_with_clap(
                wav_data, sample_rate, processor, model
            )

            print("SPEAKING RATIO:", round(speaking_ratio, 2))

            # Si m√°s del 70% del audio es "Speaking", se transcribe con Whisper
            if speaking_ratio > 0.70:
                print(
                    f"M√°s del 70% del audio es 'Speaking'. Aplicando Whisper ({WHISPER_MODEL})..."
                )
                transcription, elapsed_time = transcribe_audio(audio_path)
                if (
                    transcription and transcription.strip()
                ):  # Verifica que haya una transcripci√≥n v√°lida
                    update_transcripts_mixto_speaking(
                        video_id, WHISPER_MODEL, elapsed_time, transcription, conn
                    )
                else:
                    print(
                        f"Whisper no gener√≥ transcripci√≥n v√°lida para {audio_path} dentro del caso mixto. No se actualizar√° la BD."
                    )
            else:
                print(
                    f"üîÄ Menos del 70% es 'Speaking'. Aplicando procesamiento combinado..."
                )

                start_time = time.time()

                # Fusionar segmentos de audio clasificados como "Speaking"
                merged_audio = merge_speaking_segments(
                    speaking_segments, sample_rate, chunk_size=3, overlap=1
                )

                # Transcribir el audio fusionado si es posible
                cleaned_audio = transcribe_speaking_audio(
                    merged_audio, video_id, sample_rate
                )

                # Extraer texto con OCR y limpiarlo con GPT
                total_duration = get_video_duration(video_id, conn)
                filtered_speaking_segments = [
                    (start, end) for start, end, _ in speaking_segments
                ]  # Filtra solo los primeros dos valores
                non_speaking_segments = get_non_speaking_segments(
                    total_duration, filtered_speaking_segments, overlap=1
                )

                # Extraer frames de las partes que no son "Speaking"
                video_path = get_video_path(video_id, VIDEO_DIR, conn)
                save_frame(
                    video_id,
                    video_path,
                    FRAMES_DIR,
                    non_speaking_segments=non_speaking_segments,
                )

                # Procesar los frames con OCR
                ocr_text = procesar_frames_de_video(video_id, video_path, FRAMES_DIR)

                # Si se extrajo texto con OCR, lo guardamos en la BD
                if ocr_text:
                    update_ocr_transcripts(video_id, ocr_text, conn)

                # Aplicar GPT para limpiar el texto extra√≠do por OCR
                texto_ocr = get_text_from_ocr(video_id, conn)
                cleaned_text = procesar_texto(
                    video_id, texto_ocr, GPT_PROMPT_FRAMES, GPT_TEMPERATURE_FRAMES
                )

                print("‚úÖ Fin del procesamiento con CLAP")

                # Generar la transcripci√≥n final mixta
                video_title, transcript_audio, transcript_image = (
                    get_transcripts_mixto_puro(video_id, conn)
                )
                summary_text, prompt_tokens, completion_tokens = generate_mixto_summary(
                    video_title, transcript_audio, transcript_image
                )

                processing_time = round(
                    time.time() - start_time, 2
                )  # Calcula el tiempo de procesamiento

                update_transcripts_mixto_puro(
                    video_id,
                    processing_time,
                    transcript_audio,
                    cleaned_audio,
                    transcript_image,
                    cleaned_text,
                    summary_text,
                    conn,
                )

        else:
            print(f"‚ùó Tipo de video no reconocido o sin acci√≥n definida: {video_type}")

    except Exception as e:
        print(f"‚ùå Error general en run_music_detection: {e}")


# =======================
# EJECUCI√ìN DEL SCRIPT
# =======================

if __name__ == "__main__":
    audio_filename = "RRMM-69.wav"  # Tenemos este para pruebas de ejcuci√≥n solo de este m√≥dulo, en el flujo pub/sub no se pasa por este main.
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    conn = connect_db()
    if not os.path.exists(audio_path):
        print(f"‚ùå Error: El archivo {audio_path} no existe en {AUDIO_DIR}.")
    else:
        run_music_detection(audio_path, conn)
</file>

<file path="requirements.txt">
annotated-types==0.7.0
antlr4-python3-runtime==4.9.3
anyio==4.9.0
cachetools==5.5.2
certifi==2025.1.31
cffi==1.17.1
charset-normalizer==3.4.1
cloudpickle==3.1.1
coloredlogs==15.0.1
demucs==4.0.1
Deprecated==1.2.18
distro==1.9.0
dora_search==0.1.12
dotenv==0.9.9
easyocr==1.7.2
einops==0.8.1
filelock==3.18.0
flask==3.1.0
flatbuffers==25.2.10
fsspec==2025.3.0
google-api-core==2.24.2
google-auth==2.38.0
google-cloud-pubsub==2.29.0
googleapis-common-protos==1.69.2
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.29.3
humanfriendly==10.0
idna==3.10
imageio==2.37.0
importlib_metadata==8.6.1
Jinja2==3.1.6
jiter==0.9.0
julius==0.2.7
lameenc==1.8.1
lazy_loader==0.4
llvmlite==0.44.0
MarkupSafe==3.0.2
more-itertools==10.6.0
mpmath==1.3.0
networkx==3.4.2
ninja==1.11.1.3
numba==0.61.0
numpy==2.1.3
omegaconf==2.3.0
onnxruntime==1.21.0
openai==1.68.0
openai-whisper @ git+https://github.com/openai/whisper.git@517a43ecd132a2089d85f4ebc044728a71d49f6e
opencv-python-headless==4.11.0.86
opentelemetry-api==1.31.1
opentelemetry-sdk==1.31.1
opentelemetry-semantic-conventions==0.52b1
openunmix==1.3.0
packaging==24.2
pandas==2.2.3
pillow==11.1.0
proto-plus==1.26.1
protobuf==5.29.4
psycopg2-binary==2.9.10
pyasn1==0.6.1
pyasn1_modules==0.4.1
pyclipper==1.3.0.post6
pycparser==2.22
pydantic==2.10.6
pydantic_core==2.27.2
python-bidi==0.6.6
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2025.1
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.3
retrying==1.3.4
rsa==4.9
safetensors==0.5.3
scikit-image==0.25.2
scipy==1.15.2
setuptools==78.1.0
shapely==2.0.7
silero-vad==5.1.2
six==1.17.0
sniffio==1.3.1
sounddevice==0.5.1
SQLAlchemy==2.0.39
submitit==1.5.2
sympy==1.13.1
tifffile==2025.3.13
tiktoken==0.9.0
tokenizers==0.21.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tqdm==4.67.1
transformers==4.49.0
treetable==0.2.5
typing_extensions==4.12.2
tzdata==2025.1
urllib3==2.3.0
wrapt==1.17.2
yt-dlp==2025.2.19
zipp==3.21.0
</file>

<file path="src/classify_categories.py">
"""
	1.	Obtiene el video_id m√°s reciente de la tabla videos.
	2.	Obtiene la transcripci√≥n m√°s reciente de la tabla transcripts.
	3.	Consulta GPT con el prompt din√°mico (que incluye la lista actualizada de categor√≠as).
	4.	Recibe la respuesta en JSON con keywords y categories.
	5.	Inserta los resultados en la base de datos:
	    ‚Ä¢	Guarda las palabras clave en video_keywords.
	    ‚Ä¢	Guarda las categor√≠as en video_categories, referenciando la tabla category.
    """


import os, json, time, sys, re
from openai import AzureOpenAI
from typing import Optional, Dict, Any, List, Union
from dotenv import load_dotenv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.utils_db import connect_db, get_categories, get_final_transcript, store_keywords_categories, store_suggested_title_description

# ==================== 
# CARGAR CONFIGURACI√ìN 
# ====================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
AUDIO_DIR = os.path.join(BASE_DIR, "tmp/audio")
CONFIG_PATH = os.path.join(BASE_DIR, "config", "settings.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

GPT_TEMPERATURE_CATEGORIES = config["gpt"]["temperature_categories"]
GPT_PROMPT_CATEGORIES = config["gpt"]["prompt_categories"]

# =======================
# CONFIGURACI√ìN AZURE
# =======================
load_dotenv()
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Checkear variables
if not all([endpoint, deployment, api_key, api_version]):
    raise ValueError("‚ùå Error al cargar variables de entorno para Azure OpenAI.")

# Iniciar cliente de Azure OpenAI
client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version=api_version
)

# ==================== 
# PROCESAMIENTO DE TEXTO 
# ====================
def extract_json_block(text: str) -> str:
    """
    Extrae el bloque JSON de una respuesta con formato Markdown.

    Args:
        text (str): Texto que puede contener un bloque en formato Markdown con JSON.

    Returns:
        str: El contenido JSON como string, limpio de delimitadores Markdown.
    """
    match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        return match.group(1)
    return text.strip()  # fallback por si no hay bloque raruno

def obtain_keywords_categories(final_transcript: str, video_title: str, categories: List[str]
                               ) -> Optional[Dict[str, Union[str, float, int, List[str]]]]:
    """
    Llama a Azure OpenAI para obtener keywords y categor√≠as relevantes de un video.

    Args:
        final_transcript (str): Transcripci√≥n completa del video.
        video_title (str): T√≠tulo del video original.
        categories (List[str]): Lista de categor√≠as posibles.

    Returns:
        Optional[Dict[str, Union[str, float, int, List[str]]]]: Diccionario con resultados y metadatos,
            o None si la llamada a OpenAI falla.
    """
    categories_str = ", ".join(categories)
    prompt_template = GPT_PROMPT_CATEGORIES
    prompt = (
        prompt_template
        .replace("{{CATEGORIES}}", categories_str)
        .replace("{video_title}", video_title)
        .replace("{transcript}", final_transcript)
    )
    temperature = GPT_TEMPERATURE_CATEGORIES

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[{"role": "system", "content": prompt}],
            temperature=temperature
        )

        content = response.choices[0].message.content.strip()
        # Para limpiar el contenido si viene envuelto en el triple backtick ese
        cleaned = extract_json_block(content)
        parsed = json.loads(cleaned)
        print(parsed)

        usage = response.usage
        return {
            "keywords": parsed.get("keywords", []),
            "categories": parsed.get("categories", []),
            "prompt": prompt,
            "temperature": temperature,
            "tokens_input": usage.prompt_tokens,
            "tokens_output": usage.completion_tokens
        }
    
    except Exception as e:
        print(f"‚ùå Error llamando a Azure OpenAI: {e}")
        return None
    
def suggest_title_description(video_title: str, final_transcript: str) -> Optional[Dict[str, str]]:
    """
    Solicita a Azure OpenAI un nuevo t√≠tulo y descripci√≥n atractiva para redes sociales.

    Args:
        video_title (str): T√≠tulo original del video.
        final_transcript (str): Transcripci√≥n completa del video.

    Returns:
        Optional[Dict[str, str]]: Diccionario con claves 'title' y 'description',
        o None si la llamada falla.
    """
    prompt = (
        "Tengo un video con el siguiente t√≠tulo: \"{title}\"\n\n"
        "Y esta es su transcripci√≥n:\n\"{transcript}\"\n\n"
        "Por favor, sugi√©reme:\n"
        "1. Un nuevo t√≠tulo m√°s atractivo y claro para redes sociales.\n"
        "2. Una breve descripci√≥n (m√°ximo 250 caracteres) que resuma el contenido de forma atractiva.\n\n"
        "Devu√©lveme la respuesta en formato JSON con las claves:\n"
        "- \"title\"\n"
        "- \"description\""
    ).format(title=video_title, transcript=final_transcript)

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[{"role": "system", "content": prompt}],
            temperature=0.6
        )

        content = response.choices[0].message.content.strip()
        cleaned = extract_json_block(content)
        return json.loads(cleaned)

    except Exception as e:
        print(f"‚ùå Error llamando a Azure OpenAI para sugerencia de t√≠tulo y descripci√≥n: {e}")
        return None

# ====================
# FUNCI√ìN PRINCIPAL
# ====================

def run_classification(conn, audio_path: str, operation: str) -> Optional[Dict[str, Any]]:
    """
    Ejecuta el flujo de clasificaci√≥n de un archivo de audio: obtenci√≥n de keywords, categor√≠as,
    t√≠tulo sugerido y descripci√≥n usando Azure OpenAI, y almacenamiento de resultados.

    Args:
        audio_path (str): Ruta al archivo de audio.
        conn: Conexi√≥n abierta a la base de datos.
        operation (str): Tipo de operaci√≥n (create, modify, delete).

    Returns:
        Optional[Dict[str, Any]]: Diccionario con resultados de GPT si el procesamiento fue exitoso, None si fall√≥.
    """

    print(f"\nClasificando categor√≠as para el audio: {audio_path}")

    try:
        filename = os.path.basename(audio_path)
        print(filename)

        final_transcript, video_title, video_id = get_final_transcript(filename, conn)
        print(final_transcript, video_title, video_id)

        if not final_transcript or not video_title or not video_id:
            print("‚ùå No se encontraron datos suficientes para procesar.")
            return

        categories = get_categories(conn)

        # -------- GPT: Keywords y Categor√≠as --------
        gpt_response = obtain_keywords_categories(final_transcript, video_title, categories)

        if gpt_response:
            print(gpt_response)

            store_keywords_categories(
                conn,
                video_id,
                gpt_response["keywords"],
                gpt_response["categories"],
                gpt_response["prompt"],
                gpt_response["temperature"],
                gpt_response["tokens_input"],
                gpt_response["tokens_output"],
                operation
            )

            print("Keywords y categor√≠as almacenadas.")
        else:
            print("‚ùå No se obtuvo respuesta v√°lida para keywords/categor√≠as.")

        # -------- GPT: T√≠tulo sugerido y descripci√≥n--------
        title_description = suggest_title_description(video_title, final_transcript)
        if title_description:
            print(f"T√≠tulo y descripci√≥n sugeridos: {title_description}")
            store_suggested_title_description(conn, video_id, title_description["title"], title_description["description"])
            print("T√≠tulo y descripci√≥n almacenados.")
        else:
            print("No se obtuvo t√≠tulo ni descripci√≥n.")
        
        return {"gpt_response": gpt_response, "title_description": title_description}

    except Exception as e:
        print(f"‚ùå Error en la ejecuci√≥n de run_classification: {e}")


# Para ejecuci√≥n directa
if __name__ == "__main__":
    audio_filename = "RRMM-69.wav" # Tenemos este para pruebas de ejcuci√≥n solo de este m√≥dulo, en el flujo pub/sub se ejecuta el audio que se pasas desde el listener.
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    operation = "modify"
    conn = connect_db()
    if not os.path.exists(audio_path):
        print(f"‚ùå Error: El archivo {audio_path} no existe en {AUDIO_DIR}.")
    else:
        run_classification(conn, audio_path, operation)
</file>

<file path="src/utils_db.py">
import os
import psycopg2
from psycopg2 import IntegrityError
from urllib.parse import quote_plus
from dotenv import load_dotenv
from src.utils_error import (format_success, format_error, error_database_connection, error_pipeline_failed,
                             error_video_id_exists, pipeline_aborted_by_deletion,
                             error_video_id_to_modify_not_found, error_video_id_to_delete_not_found
                            )

# =======================
# CONFIGURACI√ìN BASE DE DATOS
# =======================
load_dotenv()

db_user = os.getenv("DB_USER")
db_password = os.getenv("DB_PASSWORD")
db_host = os.getenv("DB_HOST")
db_port = os.getenv("DB_PORT")
db_name = os.getenv("DB_NAME")
db_sslmode = os.getenv("DB_SSLMODE")

required_vars = [db_user, db_password, db_host, db_port, db_name]
if not all(required_vars):
    raise ValueError("‚ùå Error al cargar las varaibles de la base de datos.")

db_password_encoded = quote_plus(db_password)
DATABASE_URL = f"postgresql://{db_user}:{db_password_encoded}@{db_host}:{db_port}/{db_name}?sslmode={db_sslmode}"

# =======================
# CONEXI√ìN BASE DE DATOS
# =======================
def connect_db() -> psycopg2.extensions.connection | None:
    """
    Establece una conexi√≥n con la base de datos PostgreSQL utilizando psycopg2.

    Returns:
        connection | None: Objeto de conexi√≥n si tiene √©xito, None si ocurre un error.
    """
    try:
        print("üü° Intentando conectar a la base de datos...")
        conn = psycopg2.connect(DATABASE_URL)
        conn.autocommit = True  # Evita bloqueos en transacciones
        print("‚úÖ Conexi√≥n establecida correctamente con la base de datos.")
        return conn
    except Exception as e:
        print(f"‚ùå Error conectando a la base de datos: {e}")
        return None

# =======================
# INSERCI√ìN INICIAL DE DATOS (download_videos.py)
# =======================

def insert_video_metadata(metadata: dict, operation: str) -> dict:
    """
    Inserta, modifica o elimina los metadatos de un video en la base de datos, 
    en funci√≥n del tipo de operaci√≥n especificada.

    Args:
        metadata (dict): Diccionario con los metadatos del video.
        operation (str): Operaci√≥n a realizar: 'create', 'modify' o 'delete'.

    Returns:
        dict: Respuesta formateada con √©xito o error, seg√∫n el resultado de la operaci√≥n.
    """
    print("üì• Metadatos recibidos:")
    for k, v in metadata.items():
        print(f"  - {k}: {v}")
    conn = connect_db()
    if conn is None:
        print("‚ùå No se pudo establecer conexi√≥n con la BD. Metadatos no insertados.")
        return error_database_connection()

    filename_db = metadata.get("video_filename", "desconocido")

    try:
        cur = conn.cursor()

        if operation == "create":
            try:
                print(f"Insertando nuevo video '{filename_db}'...")
                cur.execute("""
                    INSERT INTO videos (
                        filename, video_url, size_mb, video_title, video_description, 
                        video_author, video_duration, video_created_at, processing_time_video_to_audio
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), %s);
                """, (
                    filename_db,
                    metadata.get("url", "sin_url"),
                    metadata.get("video_size_MB", 0),
                    metadata.get("video_title", "sin_t√≠tulo"),
                    metadata.get("video_description", "sin_descripci√≥n"),
                    metadata.get("video_author", "desconocido"),
                    metadata.get("video_duration_seconds", 0),
                    metadata.get("processing_time_video_to_audio", 0)
                ))
            except IntegrityError:
                conn.rollback()
                print(f"‚ö†Ô∏è Video '{filename_db}' ya existe en la base de datos.")
                return error_video_id_exists(filename_db)

        elif operation == "modify":
            cur.execute("SELECT video_id FROM videos WHERE filename = %s", (filename_db,))
            result = cur.fetchone()
            if not result:
                conn.rollback()
                return error_video_id_to_modify_not_found(filename_db)

            print(f"Modificando video '{filename_db}'...")
            cur.execute("""
                UPDATE videos
                SET video_url = %s,
                    size_mb = %s,
                    video_title = %s,
                    video_description = %s,
                    video_author = %s,
                    video_duration = %s,
                    video_created_at = NOW(),
                    processing_time_video_to_audio = %s
                WHERE filename = %s;
            """, (
                metadata.get("url", "sin_url"),
                metadata.get("video_size_MB", 0),
                metadata.get("video_title", "sin_t√≠tulo"),
                metadata.get("video_description", "sin_descripci√≥n"),
                metadata.get("video_author", "desconocido"),
                metadata.get("video_duration_seconds", 0),
                metadata.get("processing_time_video_to_audio", 0),
                filename_db
            ))

        elif operation == "delete":
            cur.execute("SELECT video_id FROM videos WHERE filename = %s", (filename_db,))
            result = cur.fetchone()
            if not result:
                conn.rollback()
                return error_video_id_to_delete_not_found(filename_db)

            video_id = result[0]
            print(f"üóëÔ∏è Eliminando video '{filename_db}' y registros relacionados (ID: {video_id})...")
            cur.execute("DELETE FROM video_keywords WHERE video_id = %s", (video_id,))
            cur.execute("DELETE FROM video_categories WHERE video_id = %s", (video_id,))
            cur.execute("DELETE FROM transcripts WHERE video_id = %s", (video_id,))
            cur.execute("DELETE FROM videos WHERE filename = %s", (filename_db,))
            conn.commit()
            return pipeline_aborted_by_deletion(filename_db)

        else:
            print(f"‚ùå Operaci√≥n no v√°lida: '{operation}'")
            conn.rollback()
            return format_error(
                message=f"Operaci√≥n no v√°lida: '{operation}'. Usa 'create', 'modify' o 'delete'.",
                error_code="INVALID_OPERATION"
            )

        conn.commit()
        print(f"‚úÖ Operaci√≥n '{operation}' completada correctamente para '{filename_db}'.")
        return format_success(f"Operaci√≥n '{operation}' completada correctamente.", url=metadata.get("url"))

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error en operaci√≥n '{operation}' para '{filename_db}': {e}")
        return error_pipeline_failed(url=metadata.get("url"), details=str(e))

    finally:
        conn.close()

def update_video_audio_processing(filename: str, processing_time: float, conn: psycopg2.extensions.connection) -> None:
    """
    Actualiza el campo 'processing_time_video_to_audio' en la base de datos 
    para un video espec√≠fico.

    Args:
        filename (str): Ruta del archivo de video o audio procesado.
        processing_time (float): Tiempo que tard√≥ el procesamiento (en segundos).
        conn (connection): Conexi√≥n activa a la base de datos.
    """
    filename_db = os.path.basename(filename)

    try:
        with conn.cursor() as cursor:
            query = "UPDATE videos SET processing_time_video_to_audio = %s WHERE filename = %s;"
            cursor.execute(query, (processing_time, filename_db))
            print(f"Tiempo de procesamiento actualizado para {filename_db}.")
    except Exception as e:
        print(f"‚ùå Error actualizando el tiempo de conversi√≥n en la BD: {e}")


# =======================
# INSERCI√ìN DE VOZ/M√öSICA (detect_music.py)
# =======================

def update_video_audio_analysis(filename: str,has_voice: bool,has_music: bool,conn: psycopg2.extensions.connection) -> None:
    """
    Actualiza los campos 'voice_detected', 'music_detected' y 'video_type' 
    en la base de datos para un archivo espec√≠fico.

    Args:
        filename (str): Nombre del archivo base (sin extensi√≥n o relativo).
        has_voice (bool): Indica si se detect√≥ voz.
        has_music (bool): Indica si se detect√≥ m√∫sica.
        conn (connection): Conexi√≥n activa a la base de datos.
    """

    # Convertir filename al formato esperado en la BD (con la extensi√≥n)
    filename_db = os.path.splitext(filename)[0] + ".mp4"

    # Determinar el tipo de video
    if has_voice and has_music:
        video_type = "Mixto"
    elif has_voice and not has_music:
        video_type = "Narraci√≥n"
    elif not has_voice and has_music:
        video_type = "Instrumental"
    else:
        video_type = "No_categorizable"

    print(f"\nActualizando detecci√≥n de audio en {filename} -> Voice: {has_voice}, Music: {has_music}, Type: {video_type}")

    try:
        with conn.cursor() as cursor:
            # Verificar si el video ya existe en la BD
            cursor.execute("SELECT 1 FROM videos WHERE filename = %s;", (filename_db,))
            exists = cursor.fetchone()

            if exists:
                cursor.execute("""
                    UPDATE videos
                    SET voice_detected = %s,
                        music_detected = %s,
                        video_type = %s
                    WHERE filename = %s;
                """, (has_voice, has_music, video_type, filename_db))

        conn.commit()
        print("Datos de an√°lisis de audio actualizados correctamente en la BD.")

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error actualizando datos en la BD: {e}")


def get_video_info(filename: str, conn: psycopg2.extensions.connection) -> tuple[int | None, str | None]:
    """
    Recupera el video_id y el tipo de video desde la base de datos usando el nombre del archivo.

    Args:
        filename (str): Nombre del archivo base (sin extensi√≥n).
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        tuple: video_id (int | None) y video_type (str | None), o (None, None) si no se encuentra.
    """
    filename_db = os.path.splitext(filename)[0] + ".mp4"

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT video_id, video_type FROM videos WHERE filename = %s", (filename_db,))
            result = cursor.fetchone()
            if result:
                print(f"video_id = {result[0]}, video_type = {result[1]}")
                return result[0], result[1]
            else:
                print("No se encontr√≥ el video en la base de datos.")
                return None, None
    except Exception as e:
        print(f"‚ùå Error obteniendo informaci√≥n del video desde la BD: {e}, confirmar inserci√≥n de video_id y video_type")
        return None, None

# =======================
# TRANSCRIPCI√ìN  NARRACI√ìN (detect_music.py)
# =======================

def update_transcripts_narracion(video_id: int, model_name: str, processing_time: float, transcript_text: str, conn: psycopg2.extensions.connection) -> None:
    """
    Inserta o actualiza la transcripci√≥n para un video en la base de datos.

    La funci√≥n guarda los mismos datos en los campos:
    - transcript_raw_audio
    - transcript_clean_audio
    - transcript_final

    Args:
        video_id (int): ID del video en la base de datos.
        model_name (str): Nombre del modelo de transcripci√≥n utilizado.
        processing_time (float): Tiempo de procesamiento (en segundos).
        transcript_text (str): Texto de la transcripci√≥n.
        conn (connection): Conexi√≥n activa a la base de datos.
    """
    print(f"\nInsertando o actualizando transcripci√≥n para video_id = {video_id}")

    try:
        with conn.cursor() as cursor:
            # Insertar o actualizar la transcripci√≥n
            query = """
            INSERT INTO transcripts (
                video_id, 
                transcript_model, 
                processing_time_transcription, 
                transcript_raw_audio, 
                transcript_clean_audio, 
                transcript_final,
                transcript_created_at
            ) 
            VALUES (%s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT (video_id) DO UPDATE SET
                transcript_model = EXCLUDED.transcript_model,
                processing_time_transcription = EXCLUDED.processing_time_transcription,
                transcript_raw_audio = EXCLUDED.transcript_raw_audio,
                transcript_clean_audio = EXCLUDED.transcript_clean_audio,
                transcript_final = EXCLUDED.transcript_final,
                transcript_created_at = NOW();
            """
            cursor.execute(query, (
                video_id, model_name, processing_time,
                transcript_text, transcript_text, transcript_text
            ))
            conn.commit()
            print("Transcripci√≥n insertada o actualizada correctamente.")
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error insertando transcripci√≥n en la BD: {e}")

# =======================
# TRANSCRIPCI√ìN OCR (detect_music.py)
# =======================

def get_video_path(video_id: int, VIDEO_DIR: str, conn: psycopg2.extensions.connection) -> str | None:
    """
    Obtiene la ruta completa del archivo de video asociada a un video_id.

    Args:
        video_id (int): ID del video en la base de datos.
        VIDEO_DIR (str): Ruta base donde se encuentran los videos.
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        str | None: Ruta completa del archivo o None si no se encuentra.
    """
    try:
        with conn.cursor() as cursor:
            query = "SELECT filename FROM videos WHERE video_id = %s;"
            cursor.execute(query, (video_id,))
            result = cursor.fetchone()
            if result:
                video_filename = result[0]
                video_path = os.path.join(VIDEO_DIR, video_filename)
                return video_path
            else:
                print(f"No se encontr√≥ video para video_id {video_id}")
                return None
    except Exception as e:
        print(f"‚ùå Error obteniendo ruta del video: {e}")
        return None

def update_ocr_transcripts(video_id: int, ocr_text: str, conn: psycopg2.extensions.connection) -> bool:
    """
    Inserta o actualiza el campo 'transcript_raw_image' en la tabla transcripts 
    para el video especificado. Se marca como modelo 'OCR' con tiempo 0.
    Si es la primera vez que se usa, a√±ade una restricci√≥n UNIQUE sobre video_id.

    Args:
        video_id (int): ID del video.
        ocr_text (str): Texto extra√≠do del an√°lisis OCR.
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        bool: True si se ejecut√≥ correctamente, False si hubo error.
    """
    try:
        with conn.cursor() as cursor:
            # Insertar o actualizar el transcript OCR
            cursor.execute("""
                INSERT INTO transcripts (
                    video_id,
                    transcript_model,
                    processing_time_image_recognition,
                    transcript_raw_image
                ) VALUES (%s, %s, %s, %s)
                ON CONFLICT (video_id) DO UPDATE SET
                    transcript_model = EXCLUDED.transcript_model,
                    processing_time_image_recognition = EXCLUDED.processing_time_image_recognition,
                    transcript_raw_image = EXCLUDED.transcript_raw_image;
            """, (video_id, 'OCR', 0, ocr_text))

        conn.commit()
        print("OCR guardado o actualizado correctamente en la base de datos.")
        return True

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error insertando o actualizando OCR en la BD: {e}")
        return False

def get_text_from_ocr(video_id: int, conn: psycopg2.extensions.connection) -> str | None:
    """
    Obtiene el campo 'transcript_raw_image' desde la tabla transcripts para un video.

    Args:
        video_id (int): ID del video.
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        str | None: Texto OCR sin limpiar, o None si no se encuentra o est√° vac√≠o.
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT transcript_raw_image FROM transcripts WHERE video_id = %s;", (video_id,))
            result = cursor.fetchone()
            return result[0] if result and result[0].strip() else None
    except Exception as e:
        print(f"‚ùå Error obteniendo texto OCR desde la BD: {e}")
        return None

def update_clean_ocr_transcripts(video_id: int,clean_text: str,processing_time: float,conn: psycopg2.extensions.connection) -> bool:
    """
    Inserta o actualiza el campo 'transcript_clean_image' y 'transcript_final'
    junto con el tiempo de procesamiento OCR en la base de datos.

    Args:
        video_id (int): ID del video.
        clean_text (str): Texto limpio tras el procesamiento OCR.
        processing_time (float): Tiempo que tom√≥ limpiar el texto (en segundos).
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        bool: True si la actualizaci√≥n fue exitosa, False si ocurri√≥ un error.
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO transcripts (
                    video_id,
                    processing_time_image_recognition, 
                    transcript_clean_image,
                    transcript_final,
                    transcript_created_at
                ) VALUES (%s, %s, %s, %s, NOW())
                ON CONFLICT (video_id) DO UPDATE SET
                    processing_time_image_recognition = EXCLUDED.processing_time_image_recognition,
                    transcript_clean_image = EXCLUDED.transcript_clean_image,
                    transcript_final = EXCLUDED.transcript_final,
                    transcript_created_at = NOW();
            """, (video_id, processing_time, clean_text, clean_text))

        conn.commit()
        print("Texto limpio y tiempo de OCR guardados correctamente en la base de datos.")
        return True

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error insertando o actualizando texto limpio y tiempo OCR en la BD: {e}")
        return False

# =======================
# TRANSCRIPCI√ìN MIXTO (detect_music.py)
# =======================

def get_video_duration(video_id: int, conn: psycopg2.extensions.connection) -> float | None:
    """
    Recupera la duraci√≥n del video en segundos desde la base de datos.

    Args:
        video_id (int): ID del video.
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        float | None: Duraci√≥n del video en segundos, o None si no se encuentra.
    """
    cursor = conn.cursor()
    cursor.execute("SELECT video_duration FROM videos WHERE video_id = %s;", (video_id,))
    result = cursor.fetchone()
    return result[0] if result else None  # Retorna la duraci√≥n en segundos o None si no existe

def update_transcripts_mixto_speaking(video_id: int, model_name: str, processing_time: float, transcript_text: str, conn) -> bool:
    """
    Inserta o actualiza la transcripci√≥n de voz para videos mixtos con voz detectada.
    Tambi√©n asegura que se actualice transcript_final para ser usada en clasificaci√≥n.
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO transcripts (
                    video_id,
                    transcript_model, 
                    processing_time_transcription, 
                    transcript_raw_audio, 
                    transcript_clean_audio,
                    transcript_final,
                    transcript_created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (video_id) DO UPDATE SET
                    transcript_model = EXCLUDED.transcript_model,
                    processing_time_transcription = EXCLUDED.processing_time_transcription,
                    transcript_raw_audio = EXCLUDED.transcript_raw_audio,
                    transcript_clean_audio = EXCLUDED.transcript_clean_audio,
                    transcript_final = EXCLUDED.transcript_final,
                    transcript_created_at = NOW();
            """, (video_id, model_name, processing_time, transcript_text, transcript_text, transcript_text))

        conn.commit()
        print("üìù Transcripci√≥n (Mixto-Speaking) actualizada correctamente en la base de datos.")
        return True

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error insertando o actualizando transcripci√≥n en la BD: {e}")
        return False
    

def get_transcripts_mixto_puro(video_id: int, conn: psycopg2.extensions.connection) -> tuple[str | None, str | None, str | None]:
    """
    Recupera el t√≠tulo del video y sus transcripciones limpias (audio e imagen) 
    para videos mixtos sin voz.

    Args:
        video_id (int): ID del video.
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        tuple: (video_title, transcript_clean_audio, transcript_clean_image) o (None, None, None) si no existe.
    """
    try:
        with conn.cursor() as cursor:
            query = """
                SELECT v.video_title, t.transcript_clean_audio, t.transcript_clean_image
                FROM videos v
                LEFT JOIN transcripts t ON v.video_id = t.video_id
                WHERE v.video_id = %s;
            """
            cursor.execute(query, (video_id,))
            result = cursor.fetchone()

            if result:
                video_title, transcript_audio, transcript_image = result
                return video_title, transcript_audio, transcript_image
            else:
                print(f"‚ùå No se encontraron transcripciones para video_id = {video_id}")
                return None, None, None

    except Exception as e:
        print(f"‚ùå Error obteniendo transcripciones en la BD: {e}")
        return None, None, None
        
def update_transcripts_mixto_puro(video_id: int, processing_time: float, transcript_audio: str, transcript_clean_audio: str, transcript_image: str, transcript_clean_image: str, summary_text: str, conn: psycopg2.extensions.connection) -> None:
    """
    Inserta o actualiza todas las transcripciones para videos mixtos sin voz: 
    texto de audio, imagen y resumen final (usualmente generado por LLM).

    Args:
        video_id (int): ID del video.
        processing_time (float): Tiempo total de ejecuci√≥n.
        transcript_audio (str): Texto de audio sin limpiar.
        transcript_clean_audio (str): Texto de audio limpio.
        transcript_image (str): Texto de imagen sin limpiar.
        transcript_clean_image (str): Texto de imagen limpio.
        summary_text (str): Texto resumen generado.
        conn (connection): Conexi√≥n activa a la base de datos.
    """
    try:
        with conn.cursor() as cursor:
            # Si transcript_audio o transcript_clean_audio son tuplas, extraer solo el texto
            if isinstance(transcript_audio, tuple):
                transcript_audio = transcript_audio[0]
            if isinstance(transcript_clean_audio, tuple):
                transcript_clean_audio = transcript_clean_audio[0]

            cursor.execute("""
                INSERT INTO transcripts (
                    video_id, 
                    transcript_model, 
                    processing_time_transcription, 
                    transcript_raw_audio, 
                    transcript_clean_audio,
                    transcript_raw_image,
                    transcript_clean_image,
                    transcript_final,
                    transcript_created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (video_id) DO UPDATE SET
                    transcript_model = EXCLUDED.transcript_model,
                    processing_time_transcription = EXCLUDED.processing_time_transcription,
                    transcript_raw_audio = EXCLUDED.transcript_raw_audio,
                    transcript_clean_audio = EXCLUDED.transcript_clean_audio,
                    transcript_raw_image = EXCLUDED.transcript_raw_image,
                    transcript_clean_image = EXCLUDED.transcript_clean_image,
                    transcript_final = EXCLUDED.transcript_final,
                    transcript_created_at = NOW();
            """, (
                video_id, "Mixto", processing_time,
                transcript_audio, transcript_clean_audio,
                transcript_image, transcript_clean_image,
                summary_text
            ))

        conn.commit()
        print("Transcripci√≥n del caso Mixto insertada o actualizada correctamente en la base de datos.")

    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error insertando transcripci√≥n del caso Mixto en la BD: {e}")


# =======================
# CLASIFICACI√ìN Y KEYWORDS (classify_categories.py)
# =======================

def get_categories(conn: psycopg2.extensions.connection) -> list[str]:
    """
    Recupera todas las categor√≠as disponibles en la base de datos.

    Args:
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        list[str]: Lista de nombres de categor√≠as.
    """
    with conn.cursor() as cursor:
        cursor.execute("SELECT name FROM category;")
        return [row[0] for row in cursor.fetchall()]

def get_final_transcript(filename: str, conn: psycopg2.extensions.connection) -> tuple[str | None, str | None, int | None]:
    """
    Obtiene el texto final de la transcripci√≥n, el t√≠tulo y el ID del video 
    a partir de su nombre de archivo.

    Args:
        filename (str): Nombre del archivo (puede ser .wav o .mp4).
        conn (connection): Conexi√≥n activa a la base de datos.

    Returns:
        tuple: (transcript_final, video_title, video_id) o (None, None, None).
    """
    try:
        filename = filename.replace(".wav", ".mp4")
        with conn.cursor() as cursor:
            # Obtener video_id y video_title desde videos
            query_video = """
                SELECT video_id, video_title
                FROM videos
                WHERE filename = %s
            """
            cursor.execute(query_video, (filename,))
            video_row = cursor.fetchone()

            if not video_row:
                return None, None, None

            video_id, video_title = video_row

            # Obtener transcript_final desde transcripts
            query_transcript = """
                SELECT transcript_final
                FROM transcripts
                WHERE video_id = %s
            """
            cursor.execute(query_transcript, (video_id,))
            transcript_row = cursor.fetchone()

            if not transcript_row:
                return None, None, None

            transcript_final = transcript_row[0]

            return transcript_final, video_title, video_id

    except Exception as e:
        print(f"‚ùå Error en get_final_transcript: {e}")
        return None, None, None

    
def store_keywords_categories(
    conn: psycopg2.extensions.connection,
    video_id: int,
    keywords: list[str],
    categories: list[str],
    prompt: str,
    temperature: float,
    tokens_input: int,
    tokens_output: int,
    operation: str
) -> None:
    """
    Inserta las palabras clave y categor√≠as detectadas para un video.
    Tambi√©n actualiza los metadatos GPT utilizados para la generaci√≥n.

    Args:
        conn (connection): Conexi√≥n activa a la base de datos.
        video_id (int): ID del video.
        keywords (list[str]): Lista de palabras clave.
        categories (list[str]): Lista de categor√≠as detectadas.
        prompt (str): Prompt usado con el modelo GPT.
        temperature (float): Par√°metro de temperatura del modelo.
        tokens_input (int): N√∫mero de tokens de entrada.
        tokens_output (int): N√∫mero de tokens de salida.
        operation (str): 'create' o 'modify'.
    """
    try:
        with conn.cursor() as cursor:

            # ===== CASO MODIFY: eliminar registros previos =====
            if operation == "modify":
                print(f"üîÅ Operaci√≥n MODIFY: eliminando registros anteriores para video_id {video_id}")
                cursor.execute("DELETE FROM video_keywords WHERE video_id = %s;", (video_id,))
                cursor.execute("DELETE FROM video_categories WHERE video_id = %s;", (video_id,))

            # ===== CASOS CREATE y MODIFY: insertar nuevos registros =====
            print(f"üíæ Insertando keywords y categor√≠as para video_id {video_id}")

            # Insertar palabras clave
            for keyword in keywords:
                cursor.execute(
                    "INSERT INTO video_keywords (video_id, keyword) VALUES (%s, %s);",
                    (video_id, keyword)
                )

            # Insertar categor√≠as (buscando ID en tabla 'category')
            for category in categories:
                cursor.execute("SELECT category_id FROM category WHERE name = %s;", (category,))
                result = cursor.fetchone()
                if result:
                    cursor.execute(
                        "INSERT INTO video_categories (video_id, category_id) VALUES (%s, %s);",
                        (video_id, result[0])
                    )

            # Actualizar metadatos GPT en la tabla transcripts
            cursor.execute(
                """
                UPDATE transcripts
                SET gpt_prompt = %s,
                    gpt_temperature = %s,
                    tokens_input = %s,
                    tokens_output = %s
                WHERE video_id = %s;
                """,
                (prompt, temperature, tokens_input, tokens_output, video_id)
            )

        conn.commit()
        print("Resultados almacenados correctamente.")

    except Exception as e:
        print(f"‚ùå Error al guardar los resultados en la base de datos: {e}")
        conn.rollback()

def store_suggested_title_description(conn: psycopg2.extensions.connection, video_id: int, title: str, description: str) -> None:
    """
    Guarda el t√≠tulo y la descripci√≥n sugeridos por un modelo para un video dado.

    Args:
        conn (connection): Conexi√≥n activa a la base de datos.
        video_id (int): ID del video.
        title (str): T√≠tulo sugerido.
        description (str): Descripci√≥n sugerida.
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute(
                """
                UPDATE videos
                SET suggested_title = %s,
                    suggested_description = %s
                WHERE video_id = %s;
                """,
                (title, description, video_id)
            )
        conn.commit()
        print("T√≠tulo y descripci√≥n sugeridos almacenados correctamente.")
    except Exception as e:
        print(f"‚ùå Error al guardar t√≠tulo y descripci√≥n sugeridos: {e}")
        conn.rollback()
</file>

<file path="healthcheck.py">
from flask import Flask, Response

app = Flask(__name__)

@app.route('/')
def health():
    print("healthchecked: return [OK]")
    return Response("OK", status=200)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8089)
</file>

<file path="setup/setup_db.py">
from sqlalchemy import create_engine, text
from sqlalchemy.exc import IntegrityError
from urllib.parse import quote_plus
import sys, os
from dotenv import load_dotenv
load_dotenv()

# =======================
# RUTAS Y CONFIGURACI√ìN
# =======================

# Configuraci√≥n DB
usuario = os.getenv("DB_USER")
contrase√±a = os.getenv("DB_PASSWORD")
host = os.getenv("DB_HOST")
puerto = os.getenv("DB_PORT")
db_nombre = os.getenv("DB_NAME")
ssl_mode = os.getenv("DB_SSLMODE", "require")

# Codificar la contrase√±a para incluirla de forma segura en la URL y a√±adir SSL
contrase√±a_encoded = quote_plus(contrase√±a)
DATABASE_URL = f"postgresql://{usuario}:{contrase√±a_encoded}@{host}:{puerto}/{db_nombre}?sslmode=require"

try:
    engine = create_engine(DATABASE_URL)
    with engine.connect() as conn:
        conn.execute(text("SELECT 1"))  # Verificar conexi√≥n
        print("Conexi√≥n a la base de datos establecida.")
except Exception as e:
    print(f"‚ùå Error de conexi√≥n a la base de datos: {e}")
    sys.exit(1)  # Abortar si no hay conexi√≥n

# =======================
# FUNCI√ìN PARA CREAR TABLAS
# =======================


def create_table(sql: str, table_name: str) -> None:
    """
    Ejecuta una instrucci√≥n SQL para crear una tabla en la base de datos.

    Args:
        sql (str): Sentencia SQL para crear la tabla.
        table_name (str): Nombre de la tabla para mostrar en consola.
    """
    try:
        with engine.connect() as conn:
            conn.execute(text(sql))
            conn.commit()
        print(f"Tabla '{table_name}' creada.")
    except Exception as e:
        print(f"‚ùå Error al crear la tabla '{table_name}':")
        print(e)

# =======================
# CREACI√ìN DE TABLAS
# =======================

def create_videos_table() -> None:
    """Crea la tabla 'videos' si no existe."""
    sql = """
    CREATE TABLE IF NOT EXISTS videos (
        video_id SERIAL PRIMARY KEY,                      
        filename VARCHAR(50) UNIQUE,                      
        video_url VARCHAR(2048),                          
        size_mb FLOAT,                                    
        video_title VARCHAR(500),                         
        video_author VARCHAR(100),                        
        video_description TEXT,                           
        video_duration FLOAT,                             
        video_created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,  
        processing_time_video_to_audio FLOAT,   
        voice_detected BOOLEAN,                           
        music_detected BOOLEAN,
        video_type VARCHAR(50),
        suggested_title VARCHAR(500),                 
        suggested_description TEXT   
    );
    """
    create_table(sql, "videos")

def create_categories_table() -> None:
    """Crea la tabla 'category' si no existe e inserta las categor√≠as iniciales si est√° vac√≠a."""
    create_sql = """
    CREATE TABLE IF NOT EXISTS category (
        category_id SERIAL PRIMARY KEY,                         
        name VARCHAR(100) UNIQUE NOT NULL                      
    );
    """

    default_categories = [
        "ALIMENTACI√ìN SALUDABLE",
        "BIENESTAR MENTAL",
        "SALUD CARDIOVASCULAR",
        "EJERCICIO F√çSICO",
        "BIENESTAR CORPORAL",
        "SALUD FEMENINA",
        "MEDITACI√ìN Y MINDFULNESS",
        "PREVENCI√ìN DEL C√ÅNCER",
        "PRIMEROS AUXILIOS",
        "SALUD VISUAL",
        "TABAQUISMO",
        "PEDIATR√çA"
    ]

    insert_sql = "INSERT INTO category (name) VALUES (:name) ON CONFLICT (name) DO NOTHING;"
    count_sql = "SELECT COUNT(*) FROM category;"

    try:
        with engine.begin() as conn:
            # Crear tabla si no existe
            conn.execute(text(create_sql))

            # Verificar si la tabla ya tiene datos
            result = conn.execute(text(count_sql))
            count = result.scalar() # Aqu√≠ nos devuelve 0 si est√° vac√≠a

            if count == 0:
                for category in default_categories:
                    conn.execute(text(insert_sql), {"name": category})
                print("Tabla 'category' inicializada con categor√≠as.")
            else:
                print("Tabla 'category' ya existe con datos. No se insertan categor√≠as.")
    except Exception as e:
        print(f"‚ùå Error creando tabla 'category' o insertando categor√≠as:")
        print(e)

def create_transcripts_table() -> None:
    """Crea la tabla 'transcripts' si no existe."""
    sql = """
    CREATE TABLE IF NOT EXISTS transcripts (
        transcript_id SERIAL PRIMARY KEY,                        
        video_id INTEGER NOT NULL REFERENCES videos(video_id) ON DELETE CASCADE,
        transcript_model VARCHAR(50),   
        processing_time_image_recognition FLOAT,                           
        processing_time_transcription FLOAT,                      
        transcript_raw_audio TEXT,                                
        transcript_clean_audio TEXT,                              
        transcript_raw_image TEXT,                                
        transcript_clean_image TEXT,                              
        transcript_final TEXT,                                    
        transcript_created_at TIMESTAMP,  
        gpt_prompt TEXT,                                          
        gpt_temperature FLOAT,                                    
        tokens_input INTEGER,                                     
        tokens_output INTEGER                      
    );
    """
    create_table(sql, "transcripts")

    add_unique_constraint_sql = """
    DO $$
    BEGIN
        IF NOT EXISTS (
            SELECT 1
            FROM pg_constraint
            WHERE conname = 'unique_video_transcript'
              AND conrelid = 'transcripts'::regclass
        ) THEN
            ALTER TABLE transcripts
            ADD CONSTRAINT unique_video_transcript UNIQUE (video_id);
        END IF;
    END
    $$;
    """
    try:
        with engine.connect() as conn:
            conn.execute(text(add_unique_constraint_sql))
            conn.commit()
        print("Restricci√≥n UNIQUE en 'video_id' asegurada en la tabla 'transcripts'.")
    except Exception as e:
        print("‚ùå Error al a√±adir restricci√≥n UNIQUE a 'transcripts':")
        print(e)

    

def create_video_keywords_table() -> None:
    """Crea la tabla 'video_keywords' si no existe."""
    sql = """
    CREATE TABLE IF NOT EXISTS video_keywords (
        video_keyword_id SERIAL PRIMARY KEY,                           
        video_id INTEGER NOT NULL REFERENCES videos(video_id) ON DELETE CASCADE,  
        keyword VARCHAR(100) NOT NULL                           
    );
    """
    create_table(sql, "video_keywords")

def create_video_categories_table() -> None:
    """Crea la tabla 'video_categories' si no existe."""
    sql = """
    CREATE TABLE IF NOT EXISTS video_categories (
        video_category_id SERIAL PRIMARY KEY,                                
        video_id INTEGER NOT NULL REFERENCES videos(video_id) ON DELETE CASCADE,  
        category_id INTEGER NOT NULL REFERENCES category(category_id) ON DELETE CASCADE,  
        UNIQUE(video_id, category_id)
    );
    """
    create_table(sql, "video_categories")

# =======================
# EJECUCI√ìN
# =======================

if __name__ == "__main__":
    print("[SETUP_DB] Iniciando setup de la base de datos ===")
    create_videos_table()    
    create_categories_table()   
    create_transcripts_table() 
    create_video_keywords_table()
    create_video_categories_table()

    print("[SETUP_DB] Setup completado ===")
</file>

<file path=".gitignore">
credentials/
.env
venv/
.conda
.vscode/
# tmp/
.DS_Store
src/__pycache__/
test/__pycache__/
models/
separated/
</file>

<file path="src/listener.py">
"""
listener.py
Escucha mensajes de una suscripci√≥n (INPUT_SUBSCRIPTION_ID),
procesa cada mensaje con callback() y publica una respuesta en un topic (RESPONSE_TOPIC_ID).
"""

import json
import os
import shutil
import sys
import threading
import time
from typing import Any, Dict

from dotenv import load_dotenv

load_dotenv()

# print("Usando credenciales en:", os.environ["GOOGLE_APPLICATION_CREDENTIALS"])
# print("Existe:", os.path.exists(os.environ["GOOGLE_APPLICATION_CREDENTIALS"]))

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from google.cloud import pubsub_v1

from src.classify_categories import run_classification as classify_categories
from src.config_loader import load_settings
from src.detect_music import run_music_detection as detect_music
from src.download_videos import run_download_pipeline as download_videos
from src.utils_db import connect_db
from src.utils_error import (
    error_database_connection,
    error_invalid_message_format,
    error_pipeline_failed,
    format_success,
)

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# =======================
# CONFIGURACI√ìN PUB/SUB
# =======================

settings = load_settings()
pubsub_config = settings.get("pubsub", {})

PROJECT_ID = pubsub_config.get("PROJECT_ID")
INPUT_SUBSCRIPTION_ID = pubsub_config.get("INPUT_SUBSCRIPTION_ID")
RESPONSE_TOPIC_ID = pubsub_config.get("RESPONSE_TOPIC_ID")


def callback(message) -> None:
    """
    Funci√≥n que se ejecuta al recibir un mensaje desde un topic de Pub/Sub.
    Decodifica, valida y pasa los datos al pipeline principal.

    Args:
        message (Message): Mensaje recibido desde Pub/Sub.

    Returns:
        None. El mensaje se confirma (ack) o se rechaza (nack) seg√∫n el resultado.
    """
    print("Entrando en funci√≥n callback")
    try:
        raw_data = message.data.decode("utf-8")
        print(f"\nMensaje recibido: {raw_data}")

        try:
            pubsub_data = json.loads(raw_data)
        except json.JSONDecodeError:
            print(
                "‚ùå Error al interpretar el mensaje: no es JSON v√°lido. Mensaje borrado."
            )
            message.ack()  # O message.nack() si queremos reintentarlo (ya veremos)
            return

        # Mostrar parte √∫til del mensaje (por ejemplo, la URL)
        print(f"Procesando video: {pubsub_data.get('url', 'Sin URL')}")

        pubsub_data["operation"] = pubsub_data["operation"].lower()
        operation = pubsub_data["operation"]

        run_pipeline(pubsub_data, operation)
        print(f"Video procesado en pipeline. Mensaje procesado.")
        message.ack()  # Confirmamos el mensaje

    except Exception as e:
        print(f"‚ùå Excepci√≥n general en callback: {e}. Mensaje rechazado.")
        message.nack()


def clean_tmp_folders() -> None:
    """
    Elimina todos los archivos y subdirectorios de las carpetas temporales del proyecto.

    Las rutas relativas que se limpian son las de la carpeta tmp/
    Las rutas son resueltas respecto a `BASE_DIR`. Si alguna carpeta no existe, se omite silenciosamente.

    Raises:
        None. Si ocurre un error al eliminar un archivo/directorio individual, se imprime pero no interrumpe la ejecuci√≥n.
    """
    tmp_folders = ["tmp/audio", "tmp/video", "tmp/stems", "tmp/frames"]
    for folder in tmp_folders:
        full_path = os.path.join(BASE_DIR, folder)
        if os.path.exists(full_path):
            for f in os.listdir(full_path):
                f_path = os.path.join(full_path, f)
                try:
                    if os.path.isfile(f_path) or os.path.islink(f_path):
                        os.remove(f_path)
                    elif os.path.isdir(f_path):
                        shutil.rmtree(f_path)
                except Exception as e:
                    print(f"No se pudo eliminar {f_path}: {e}")

    separated_dir = os.path.join(BASE_DIR, "separated")
    if os.path.exists(separated_dir):
        try:
            shutil.rmtree(separated_dir)
        except Exception as e:
            print(f"No se pudo eliminar la carpeta 'separated': {e}")

    print("Carpetas temporales limpias.")


def run_pipeline(pubsub_data: Dict[str, Any], operation: str) -> None:
    """
    Ejecuta el flujo completo de procesamiento del video a partir de datos recibidos por Pub/Sub.

    Incluye:
    - Validaci√≥n de campos.
    - Conexi√≥n a base de datos (con reintento).
    - Descarga y conversi√≥n de video a audio.
    - Detecci√≥n de m√∫sica y voz.
    - Clasificaci√≥n y generaci√≥n de metadatos con OpenAI.
    - Publicaci√≥n de respuesta en topic de salida.

    Args:
        pubsub_data (Dict[str, Any]): Datos del mensaje recibido desde Pub/Sub.
        operation (str): Tipo de operaci√≥n a realizar ('create', 'modify', 'delete').

    Returns:
        None. Publica un mensaje con el resultado final del pipeline.
    """
    print("\n=== Iniciando procesamiento del video ===\n")
    print(f"Datos recibidos en pipeline:\n{json.dumps(pubsub_data, indent=2)}")

    # Campos requeridos en el mensaje
    required_fields = ["title", "url", "author", "description", "operation"]

    missing_fields = [field for field in required_fields if field not in pubsub_data]
    if missing_fields:
        print(f"‚ùå Faltan campos en el mensaje: {missing_fields}")
        publish_response(
            error_invalid_message_format(
                details=f"Faltan campos: {', '.join(missing_fields)}"
            )
        )
        return

    conn = connect_db()
    if conn is None:
        print("No se pudo conectar a la BD. Reintentando en 5 segundos...")
        time.sleep(5)
        conn = connect_db()

    if conn is None:
        print(
            "‚ùå No se pudo establecer conexi√≥n con la BD tras reintentar. Abortando pipeline."
        )
        publish_response(error_database_connection())
        return

    try:
        download_result = download_videos(pubsub_data, conn, operation)
        # Todos los errores definidos con la estructura de format_error pillados en download_result detienen la ejecuci√≥n.
        if (
            isinstance(download_result, dict)
            and download_result.get("status") == "error"
        ):
            print(f"‚ùå Pipeline abortado: {download_result['message']}")
            publish_response(download_result)
            return

        audio_path = download_result
        if not audio_path:
            print("‚ùå No se pudo obtener el audio. Abortando pipeline.")
            publish_response(
                error_pipeline_failed(
                    url=pubsub_data.get("url"),
                    details="Fallo en la descarga o conversi√≥n de audio.",
                )
            )
            return

        print("Descarga y conversi√≥n a audio completados.\n")

        # Detectar m√∫sica y voz
        detect_music(audio_path, conn)
        print("Procesamiento de m√∫sica completado.\n")

        # Clasificar categor√≠as
        classification_result = classify_categories(conn, audio_path, operation)
        gpt_response = classification_result.get("gpt_response", {})
        title_description = classification_result.get("title_description", {})

        # Si todo fue bien, publicamos √©xito
        response = {
            "status": "success",
            "message": "Procesamiento completado con √©xito.",
            "url": pubsub_data.get("url"),
            "error_code": None,
            "keywords": gpt_response.get("keywords", []),
            "categories": gpt_response.get("categories", []),
            "suggested_title": title_description.get("title", ""),
            "suggested_description": title_description.get("description", ""),
        }
        publish_response(response)

    except Exception as e:
        print(f"‚ùå Error en el pipeline: {e}")
        publish_response(
            error_pipeline_failed(url=pubsub_data.get("url"), details=str(e))
        )

    finally:
        conn.close()
        print("Conexi√≥n con la base de datos cerrada.")
        clean_tmp_folders()


def publish_response(response: Dict[str, Any]) -> None:
    """
    Publica un mensaje de respuesta en el topic de salida de Pub/Sub.

    Args:
        response (Dict[str, Any]): Diccionario que representa la respuesta a publicar.

    Returns:
        None
    """
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path(PROJECT_ID, RESPONSE_TOPIC_ID)

    message_json = json.dumps(response).encode("utf-8")
    publisher.publish(topic_path, message_json)

    print(f"Respuesta publicada: {response}")


def log_status() -> None:
    """
    Hilo en segundo plano que imprime peri√≥dicamente un mensaje para indicar que el listener sigue activo.

    Returns:
        None
    """
    while True:
        print("listener activo...")
        time.sleep(60)


def start_listener() -> None:
    """
    Inicializa el listener de mensajes desde Pub/Sub y mantiene el proceso en escucha continua.

    Configura el control de flujo para recibir solo un mensaje a la vez y lanza un hilo secundario
    para mostrar el estado del servicio peri√≥dicamente.

    Returns:
        None
    """
    print("listener iniciado")
    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(PROJECT_ID, INPUT_SUBSCRIPTION_ID)

    flow_control = pubsub_v1.types.FlowControl(max_messages=1)

    streaming_pull_future = subscriber.subscribe(
        subscription_path, callback=callback, flow_control=flow_control
    )

    print(f"Escuchando mensajes en la suscripci√≥n: {INPUT_SUBSCRIPTION_ID}")

    status_thread = threading.Thread(target=log_status, daemon=True)
    status_thread.start()

    try:
        streaming_pull_future.result()
    except KeyboardInterrupt:
        print("\n‚ùå Listener detenido manualmente.")
        streaming_pull_future.cancel()
        streaming_pull_future.result()


if __name__ == "__main__":
    print("listener.py ejecutado directamente.")
    start_listener()
</file>

<file path="src/download_videos.py">
"""
Este m√≥dulo descarga videos, extrae metadatos y los convierte a audio.
	1.	Carga la configuraci√≥n desde config.json y establece la conexi√≥n con la base de datos.
	2.	Descarga videos desde URLs de vimeo, guardando metadatos como t√≠tulo, descripci√≥n y duraci√≥n.
	3.	Convierte los videos a audio en formato WAV.
	4.	Registra y actualiza los metadatos en la base de datos, incluyendo tiempos de procesamiento.
	5.	Selecciona videos desde un CSV y ejecuta el flujo de descarga y conversi√≥n.
    """

import os, sys, time, subprocess, re
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from datetime import datetime
from typing import Optional, Dict, Any, Union
from contextlib import redirect_stdout, redirect_stderr
import pandas as pd
import requests
from urllib.parse import urlparse
from src.utils_db import insert_video_metadata, update_video_audio_processing
from dotenv import load_dotenv


# =======================
# CONFIGURACI√ìN DIRECTORIOS
# =======================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(BASE_DIR, "tmp")
RAW_VIDEOS_DIR = os.path.join(DATA_DIR, "video")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(RAW_VIDEOS_DIR, exist_ok=True)
os.makedirs(AUDIO_DIR, exist_ok=True)

CSV_PATH = os.path.join(DATA_DIR, "videos.csv")

# =======================
# CONFIGURACI√ìN API VIMEO
# =======================
load_dotenv()
ACCESS_TOKEN = os.getenv('VIMEO_ACCESS_TOKEN')

# Cabeceras para la API
HEADERS = {'Authorization': f'Bearer {ACCESS_TOKEN}'}

# =======================
# FUNCIONES AUXILIARES
# =======================
def get_video_id(video_url: str) -> str:
    """Extrae el ID del video desde la URL de Vimeo."""
    return urlparse(video_url).path.strip("/")


def sanitize_filename(name: str) -> str:
    """Limpia el nombre del archivo eliminando caracteres no v√°lidos."""
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, "")
    return name.replace(" ", "_")


def get_video_duration(video_url: str) -> Optional[float]:
    """Obtiene la duraci√≥n de un video de Vimeo en segundos."""
    try:
        video_id = get_video_id(video_url)
        api_url = f"https://api.vimeo.com/videos/{video_id}"
        response = requests.get(api_url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()
        return round(float(data.get("duration", 0)), 2)
    except Exception as e:
        print(f"‚ùå Error obteniendo duraci√≥n del video: {e}")
        return None


# =======================
# DESCARGA Y METADATOS
# =======================
def download_video(url: str, folder: str, description: str, author: str, operation: str, pubsub_title: str):
    """
    Descarga un video desde Vimeo y guarda sus metadatos en la base de datos.

    - El nombre del archivo (.mp4) viene de Vimeo (`name`).
    - El t√≠tulo para mostrar viene del mensaje Pub/Sub (`pubsub_title`).
    """
    try:
        print("üì• Iniciando descarga de video...")

        # 1. Llamada a la API de Vimeo
        video_id = get_video_id(url)
        api_url = f"https://api.vimeo.com/videos/{video_id}"
        response = requests.get(api_url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()

        # 2. Verifica opciones de descarga
        if 'download' not in data or not data['download']:
            print("‚ö†Ô∏è No hay enlaces de descarga disponibles.")
            return None

        # 3. Extraer datos
        vimeo_name = data.get('name', 'video_descargado')
        safe_filename = sanitize_filename(vimeo_name) + '.mp4'
        video_path = os.path.join(folder, safe_filename)
        print(f"üé¨ Nombre archivo (Vimeo): {vimeo_name}")
        print(f"üìù T√≠tulo Pub/Sub: {pubsub_title}")
        print(f"üíæ Guardando como: {safe_filename}")

        # 4. Obtener mejor opci√≥n de descarga
        download_link = max(data['download'], key=lambda x: x.get('width', 0)).get('link')

        # 5. Descargar el archivo
        with requests.get(download_link, stream=True) as r:
            r.raise_for_status()
            with open(video_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print("‚úÖ Descarga completada.")

        # 6. Preparar metadatos
        metadata = {
            "video_filename": safe_filename,
            "url": url,
            "video_size_MB": round(os.path.getsize(video_path) / (1024 * 1024), 2),
            "video_title": pubsub_title,
            "video_description": description,
            "video_author": author,
            "video_duration_seconds": round(float(data.get("duration", 0)), 2),
            "processing_datetime": datetime.now(),
            "processing_time_video_to_audio": None
        }

        # 7. Insertar en base de datos
        print(f"üìÑ Insertando en base de datos: {metadata}")
        insert_result = insert_video_metadata(metadata, operation)
        if isinstance(insert_result, dict) and insert_result.get("status") == "error":
            return insert_result

        return video_path

    except Exception as e:
        print(f"‚ùå Error durante la descarga: {e}")
        return None


def convert_video_to_audio(video_path: str, audio_dir: str, conn) -> Optional[str]:
    """Convierte un video a audio WAV y registra el tiempo de procesamiento."""
    video_filename = os.path.basename(video_path)
    base_name = os.path.splitext(video_filename)[0]
    audio_filename = f"{base_name}.wav"
    audio_path = os.path.join(audio_dir, audio_filename)

    if not os.path.exists(video_path):
        print(f"‚ùå El archivo {video_filename} no existe.")
        return None

    start_time = time.time()
    try:
        subprocess.run(
            ["ffmpeg", "-v", "warning", "-i", video_path, "-ac", "1", "-ar", "16000", audio_path, "-y"],
            check=True,
        )
        elapsed_time = round(time.time() - start_time, 2)
        update_video_audio_processing(video_filename, elapsed_time, conn)
        return audio_path
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Error al convertir video a audio: {e}")
        return None


# =======================
# FLUJO PRINCIPAL
# =======================
def run_download_pipeline(pubsub_data: Dict[str, str], conn, operation: str) -> Optional[str]:
    """
    Ejecuta el flujo de descarga y conversi√≥n de video a partir de un mensaje Pub/Sub.
    Usa `title` de Pub/Sub como t√≠tulo, y el `name` de Vimeo como nombre de archivo.
    """
    title = pubsub_data.get("title")
    url = pubsub_data.get("url")
    author = pubsub_data.get("author", "Desconocido")
    description = pubsub_data.get("description", "")

    print(f"\nüöÄ Procesando entrada de Pub/Sub: {title} - {url}\n")
    if not title or not url:
        print("‚ùå Error: T√≠tulo o URL faltante.")
        return None
    print(f"üì® T√≠tulo desde Pub/Sub recibido: '{title}'")
    video_result = download_video(
    url=url,
    folder=RAW_VIDEOS_DIR,
    description=description,
    author=author,
    operation=operation,
    pubsub_title=title  # üëà nuevo argumento
    )

    if isinstance(video_result, dict) and video_result.get("status") == "error":
        return video_result

    if video_result:
        audio_path = convert_video_to_audio(video_result, AUDIO_DIR, conn)
        print("‚úÖ Video procesado correctamente.")
        return audio_path
    else:
        print("‚ùå No se pudo descargar el video.")
        return None


if __name__ == "__main__":
    print("Este script debe ejecutarse desde `listener.py` con datos de Pub/Sub.")
</file>

<file path="entrypoint.sh">
#!/bin/bash
set -e

echo "Ejecutando entrypoint.sh en host: $(hostname)" 

# Lanzar el servidor de healthcheck en segundo plano antes de nada
echo "=> Arrancando healthcheck..."
python3 healthcheck.py &

if [ "$1" = "setup" ]; then
    echo "[entrypoint] Detectado modo SETUP."
    python3 setup/setup_db.py
    echo "[entrypoint] Finalizando SETUP."
fi

if [ "$1" = "batch_videos" ]; then
    echo "[entrypoint] Detectado modo batch_videos"
    python3 setupno_pubsub/run_batch_processing.py
    echo "[entrypoint] Finalizando batch_videos."
fi

echo "=> Arrancando listener..."
python3 src/listener.py
</file>

<file path="Dockerfile">
# Usa una imagen base de Python
FROM python:3.13-slim

# Establece el directorio de trabajo dentro del contenedor
WORKDIR /app

# Puerto del healthcheck
EXPOSE 8089

# Copia los archivos del proyecto al contenedor
COPY . .

# Instala ffmpeg, git y limpia cache para reducir peso
RUN echo " * Instalando ffmpeg y git" && \
    apt-get update && \
    apt-get install -y git ffmpeg && \
    rm -rf /var/lib/apt/lists/*

# Instala las dependencias Python
RUN echo " * Instalando dependencias Python" && \
    pip install --no-cache-dir -r requirements.txt

# Ejecuta los scripts de setup necesarios (excepto el de DB)
RUN echo " * Instalando modelos IA" && \
    python3 setup/setup_clap.py && echo "   + Ejecutado setup clap" && \
    python3 setup/setup_demucs.py && echo "   + Ejecutado setup demucs" && \
    python3 setup/setup_silero_vad.py && echo "   + Ejecutado setup silero"

# Da permisos de ejecuci√≥n al entrypoint
RUN chmod +x /app/entrypoint.sh

# Comando que se ejecutar√° al arrancar el contenedor
ENTRYPOINT ["/app/entrypoint.sh"]
CMD ["batch_videos"]
</file>

</files>
